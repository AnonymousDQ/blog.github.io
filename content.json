[{"title":"分布式锁","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、分布式锁/","text":"一、分布式锁在单机场景下，可以使用Java里的内置所来实现进行同步。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁。 阻塞所通常使用互斥量来实现： 互斥量mutex为0表示有其他进程在使用锁，此时处于锁定状态。 互斥量mutex为1表示未锁定状态。 1和0可以用一个整形值表示，也可以用某个数据是否存在表示。 1、数据库的唯一索引获取锁时向表中插入一条记录，释放锁时删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否在锁定状态。 存在以下几个问题： 锁没有失效时间，解锁失败的话，其他进程无法再获得该锁。 只能是非阻塞锁，插入失败直接就报错，无法重试。 不可重入，已经获得锁的进程也必须重新获取锁。 2、Redis的SETNX指令使用SETNX（set if not exist）指令插入一个键值对，如果key已经存在，那么会返回false，否则插入成功并返回true。 SETNX指令和数据库的唯一索引类似，保证了只存在一个key的键值对，那么可以用一个key的键值对是否存在来判断是否存于锁定状态。 EXPIRE指令可以为一个键值对设置一个过期时间，从而避免了数据库唯一索引方式中释放锁失败的问题。也就是数据库的唯一索引解决方案的升级而已，只是可以设置过期时间。 3、Redis的RedLock算法使用了多个Redis实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。多个Redis实例实现，也就是即使一台Redis挂了，还可以有其他的服务可用。 尝试从N个互相独立的Redis实例中获取锁。 计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数(N/2+1)，半数以上的实例上获取锁，那么就认为获取锁成功了。 如果锁获取失败，就到每个实例上释放锁。 4、Zookeeper的有序节点1、Zookeeper抽象模型Zookeeper提供了一种树型结构的命名空间，/app1/p_1节点的父节点为app1。有点类似linux系统文件系统，文件目录树 2、节点类型 永久节点：不会因为会话结束或者超时而消失。 临时节点：如果会话结束或者超时就会消失。 有序节点：会在节点名的后面加上一个数字后缀，并且是有序的。例如生成的有序节点为/lock/node-0000000000，它的下一个有序节点就是/lock/node-0000000001，以此类推。 3、监听器为一个节点注册监听器，在节点状态发生改变的时候，会给客户端发送消息 。 4、分布式锁实现 创建一个目录/lock。 当一个客户端需要获取锁时，在/lock下创建临时的且有序的子节点。 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁。否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直到获取锁为止。 执行业务代码，完成后，删除对应的子节点。 5、会话超时如果一个已经获的锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其他会话就可以获取锁。可以看到zookeeper分布式锁不会出现数据库的唯一索引的分布式锁释放锁失败的问题。 6、羊群效应一个节点未获得锁，只需要监听自己前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其他所有子节点都会收到通知(羊群效应)，而我们秩序网它的后一个子节点收到通知。 二、分布式事务","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"操作系统","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、操作系统概述/","text":"一、操作系统概述1、指令权限与工作模式处理器能处理的指令分为： 特权指令：需要特殊权限才可以使用的指令，通常涉及系统安全。 非特权指令：任何情况下都可以使用的指令。 处理器的控制寄存器中用专门的位标识当前的工作模式： 核心态(管态)：已获得特殊权限的状态，能够执行所有指令。 用户态(目态)：只能指令非特权指令。 事实上，现代处理器通常具有更多层权限和工作模式。从高到低划分为特权级0(管态)，特权级1，特权级2，特权级3(目态)。 2、内存空间与操作系统的内核处理器处于核心态时可以将内存的不同区段分别设置为两种类型： 内核(系统)空间：其中存储的程序将运行在核心态上，从而可以使用所有指令，且可以访问所有内存空间数据。 用户空间：其中存储的程序将运行在用户态上，于是只能使用非特权指令，且只能访问用户空间数据。 操作系统通常在启动时指定内核空间，并将其核心程序放置在内核空间内。操作系统的这一部分叫内核，操作系统的其他部分叫核外部分。 3、操作系统的内核操作系统的内核通常包含以下程序和数据： 与硬件密切相关的操作 关键数据结构 基本中断处理程序 使用频繁的功能模块 操作系统的内核具有两个基本特点： 常驻内存 运行在核心态 4、固件操作系统的启动依赖于一组称作固件（Firmware）的特殊软件。 固件存储在只读存储器ROM中，也可以将部分程序或者数据存储在硬盘中。 与PC启动相关的固件有BIOS和UEFI，他们在计算机启动时首先被运行，并运行基本硬件的检查，装入操作系统引导程序，提供基本硬件驱动程序和中断处理程序等。 5、基本输入输出系统(BIOS)BIOS包含3个部分： POST自检程序：计算机启动后识别硬件配置、自检、初始化。 基本启动程序：在存储介质中查找有效的主引导程序，如果找到，将它读入内存[0000:7C00]处并开始执行，从而引导操作系统启动。 基本硬件驱动程序以及中断处理程序：用来在未加载操作系统时识别和控制基本IO设置(键盘、鼠标、显卡、硬盘等) 硬盘的主引导记录MBR 5.1、基于BIOS的操作系统启动过程 硬件自检 加载驱动 基本启动程序 主引导程序(440字节) 引导程序(512字节) 操作系统装载程序 操作系统 操作系统装载程序Loader的作用： 进一步检查和配置系统硬件。 建立内核空间，装入操作系统内核，并初始化数据。 加载用户登录程序和人机交互环境。 6、统一的可扩展固件接口UEFIIntel公司在1997年新推出的高性能处理器，计划设计一种可扩展的、标准化的固件接口规划EFI(Extensible Firmware Interface)，用来计算机系统的启动以及提供与操作系统的接口。在2006年EFI发展成UEFI(Unified EFI)。 UEFI的特点： 驱动程序环境DXE 磁盘管理采用GUID分区方法 UEFI应用程序 6.1、基于UEFI的操作系统启动过程UEFI体系包含2部分内容： 平台初始化框架：包含了支持系统启动的底层程序，包含：安全检查、硬件自检和初始化、DXE、启动管理器。 UEFI映像(UEFI Image):一组在框架的支持下可执行的程序，包括启动管理器、操作系统装载程序、临时操作系统装载程序、EFI应用程序、设备驱动程序。 UEFI固件的启动过程： 安全检查 Pre-EFI 加载DXE 启动管理器 临时操作系统装载程序 EFI应用程序 操作系统装载程序 操作系统 7、GUID硬盘分区方式UEFI采用GUID硬盘分区方式，优点是： 扇区块号为64位，消除2TB限制。 磁盘分区数不受4个限制，最多划分128个分区 分区扇区数为64位，消除2TB限制 自带备份，增加可靠性 采用CRC32校验，增加完整性 每个分区都有唯一标识GUID 8、操作系统的用户接口–命令接口命令接口是人机交互的主要途径 8.1、按命令接口在操作系统内的实现方式，分为 内部命令：内核完成的命令 外部命令：由其他应用程序完成的命令 8.2、按命令接口的使用方式，分为： 脱机命令：预先编写好，送交执行后无法立即反馈的命令接口形式。 比如批处理系统中的JCL 联机命令：交互式执行，能够立即得到反馈的命令接口形式。 比如分时操作系统的交互式命令接口。又可细分为字符命令、菜单命令和图标命令。 9、操作系统的用户接口–程序接口(系统调用)9.1、系统调用的简介程序接口是程序员在编写应用程序时使用操作系统功能的接口。目前，程序接口通常通过系统调用System Call方式实现。 系统调用在不同的上下文中，可能表示2种含义： 一组由操作系统设计者编写的内核中的子程序，用来封装程序员可能使用到的操作系统功能。 程序员在编程中对这些子程序进行调用的行为 系统调用所涉及的子程序属于操作系统内核的一部分，保存在内存空间中，应用程序不能直接访问，于是需要通过特殊方式才能实现系统调用。 9.2、系统调用的实现： 放置调用号标识 访管指令：发出表示系统调用请求的中断或者陷阱 处理器保护程序现场，转向中断处理（内核态） 中断处理程序识别中断为系统调用请求，于是转向内核的访管指令处理程序 访管指令处理程序在地址入口表中查找所请求的调用号对应的内核子程序的入口地址。 从查找到的地址开始执行内核子程序(系统调用)并得到结果。 恢复应用程序现场，返回用户态。 9.3、系统调用与用户子程序对比 对比项目 系统调用 用户子程序 运行环境 内核态（管态） 用户态(目态) 访问方式 访管指令（中断或者陷阱） 直接跳转、返回 与主程序关系 与主程序分开、独立 同一进程地址空间 共享性 可以在多个进程间共享 同一进程内部调用 二、进程管理1、程序与系统的工作流程程序应具体2个基本特点： 顺序性 可再现性 系统的工作流程分为顺序执行和并发执行 顺序执行的工作方式具有2个特点： 封闭性：独占全部资源，不受其他程序影响 可再现性：重复运行的结果相同 并发执行的工作方式中，多道程序 微观上看，轮流执行 宏观上看，同时执行 并发执行的工作方式有3个特点： 随机性：运行时情景随机 不可再现性：重复运行结果可能不同 相互制约：资源争夺和协作关系 2、进程Dijkstra：一道程序在一个数据集上的一次执行过程，叫一个进程Process IBM：程序的运行过程成为任务Task 进程和任务都是运行着程序的描述，二者可以互换使用，不加区别。 进程的主要特征： 动态性：具有生命周期，状态不断变化 并发性：可以并发执行 独立性：互相独立，只能访问自身的内存空间 结构性：不同的进程可抽象出相同的属性，从而抽象结构相同 异步性：运行时情况不可预知 3、进程的动态性进程的3种状态 就绪：能够运行但却未在被处理器处理的状态 运行：正在被处理器处理的状态 阻塞：由于各种原因(IO操作、资源缺乏等)暂时无法运行的状态。 4、进程信息的描述–PCB进程控制块PCB：是操作系统用来描述进程信息的数据结构，由3部分组成： 基本信息部分： 进程名：通常是程序名 进程标识符PID：操作系统用来识别进程的唯一标识 用户标识：创建进程的用户标识 进程状态：运行、就绪或者阻塞 管理信息部分： 程序和数据地址：程序和数据在内存中的位置信息 IO操作相关参数：进程进行IO操作时的参数 进程通信信息：用来进程间通信的消息队列指针等。 控制信息部分： 现场信息：进程离开运行状态时，保存进程现场以备恢复。 调度参数：与进程调度相关的参考信息，如优先级、运行时间等 同步、互斥信号量：用来保证进程间同步、互斥关系的信息。 5、PCB队列和进程管理操作系统用PCB来描述进程信息，并利用2种PCB队列来组织这些PCB: 就绪队列：就绪进程的PCB组成的队列，按照进程性质或者优先级，系统中可以有多个就绪队列，用来提高进程调度算法选择进程的效率。在多处理器系统中，就绪队列也叫请求队列，每个处理器都有独立的请求队列。 等待队列：阻塞进程的PCB组成队列，按照阻塞原因，系统中可以有多个等待对了。方便在阻塞原因解除的时候统一调整进程状态。 6、原语改变进程状态操作不应该被打断，也就是具有原子性。 原语是特殊的程序段，这样的程序段中的所有指令要么全部执行要么一个都不执行。原语就是具有原子性的程序段。 原语可以被看做广义的指令，指令都具有原子性 原语中的指令若执行不成功，系统需要回退到原语执行前的状态。 定义原语是为了保证系统运行的一致性。 7、进程控制–创建进程控制通过一组原语来实现，包括进程的创建、撤销、阻塞、唤醒和切换。 1、进程的创建原语Create用来创建一个进程Create可能在以下情况下被使用： 按照作业调度程序的作业步规定 按照用户提交的命令 用户程序使用了创建进程的系统调用比如Unix的fork Create被使用时完成以下操作： 建立PCB 生成进程标识符PID 初始化进程和PCB 将PCB加入适当的就绪队列 8、进程控制–撤销1、进程的撤销原语Destroy用来结束一个进程Destroy可能在以下情况下被使用： 进程执行完毕 进程出错而结束 父进程结束 人为终止 Destroy被使用时完成以下操作： 进程资源回收 删除进程PCB 9、进程控制–阻塞1、进程的阻塞原语Block用来让一个进程进入阻塞状态Block在阻塞原因发生时被使用。 Block被使用时完成以下操作： 修改进程状态为阻塞状态 将进程的现场保存，以备恢复 将进程PCB加入合适的等待队列 10、进程控制–唤醒1、进程的唤醒原语Wakeup用来让一个进程从阻塞状态进入就绪状态Wakeup在阻塞原因解除时被使用。 Wakeup被使用时完成以下操作： 将进程PCB从等待队列中移除 修改进程状态为就绪状态 将进程PCB加入合适的就绪队列 阻塞原因解除后，进程未必可以直接运行，因此合理的唤醒原语并不会将阻塞状态直接修改为运行状态。 11、进程控制–切换进程切换使处理器的控制权在进程之间专员，无法通过操作系统内核独立完成，必须借助硬件来实现。 进程：系统调用（系统调用中断），外部因素（IO中断，异常），调度维护（计时器中断），然后进程进入中断、异常处理机制。然后在通过内核功能，控制原语，进程调度等。 12、相关进程间的制约关系–间接制约间接制约关系：仅由于使用相同的资源而造成的制约。 比如：2个进程p1和p2分别使用同一台打印机，逐行打印各自文本。 1、间接制约与临界资源 在操作系统的控制下，进程对系统资源的访问逻辑上需要经历3个步骤：申请、使用、归还。 某些系统资源，在进程从申请到归还的周期中只能允许此进程独占，否则将造成运行错误。则称这样的资源为临界资源。 常见的临界资源有：打印机，存储单元，文件等。 间接制约实质上是一组并发进程共享某种临界资源的制约关系。 2、间接制约与护持关系 临界区Critical Section/Critical Region：是指进程对应的程序中访问临界资源的一段程序代码，即进程从申请资源开始到归还资源为止的一段程序代码。 对于一组(在某资源上)存在间接制约关系的并发进程，可以通过建立一种机制来避免制约关系导致的运行错误：当一个进程在这种临界资源对应的临界区内执行时，其他要求进入相关临界区的进程必须等待。 上面的机制得以建立，则称这些并发进程为互斥进程，它们之间建立了护持关系。 3、临界区管理实现互斥关系的关键是对临界区的执行进行有效控制。 临界区管理的准则有： 空闲让进：如果进程请求进入临界区，而没有其他进程在相关临界区内执行，则允许进入。 忙则等待：如果有进程在临界区内执行，则其他要求进入相关临界区的进程都需要等待。 有限等待：对于要求进入临界区的进程，等待的时间应该是有限的 让权等待：当一个进程离开临界区时，应该让请求进入相关临界区的下一个进程进入临界区执行。 4、互斥关系的实现–加锁机制加锁机制由3个要素组成：锁变量key，加锁操作lock(key)和解锁操作unlock(key) 12345678lock(key)&#123; while(key==1); key=1;&#125;unlock(key)&#123; key=0;&#125; 这个机制中，锁变量key=1时表示上锁状态，key=0为未上锁。 当进程申请资源时需要对资源加锁，若资源已经上锁，则上锁操作 将进行反复测试也就是反复等待，直到资源被解锁了；当进程归还资源的时候需要对资源解锁。 高级语言，软件实现的加锁机制并不能实现真正的加锁，因为加锁操作内部仍可能被打断。 加锁机制的缺陷： 存在忙等待现象，浪费了处理器时间 存在饥饿现象，进程等待时间可能无限长 多个锁变量的加锁操作可能导致进程死锁 5、互斥关系的实现–信号量机制信号量机制由3个要素组成：信号量s，原语p和原语v 也就是所谓的pv操作 12345678910111213141516struct semaphore&#123; int value;//信号量的值 PCB *bq;/由于该信号量而阻塞的进程队列PCB队列，(阻塞队列/等待队列)&#125;p(s)&#123; s.value=s.value-1; if(s.value&lt;0) blocked(s);&#125;v(s)&#123; s.value=s.value+1; if(s.value&lt;=0) wakeup(s);&#125; 信号量机制中，s.value&gt;0表示资源就绪，否则就表示有其他进程在使用或者请求使用资源。 进程请求资源时需要使用原语p，如果资源不空闲，则进入由于s而导致的等待队列。直到正在使用资源的进程归还资源时，使用原语v将它唤醒。 信号量机制相比于加锁机制的优势： 信号量机制把等待资源的进程设置为阻塞状态，而不是忙等待，提高了处理器的利用率。 信号量机制中由操作系统原语wakeup唤醒进程，可以采取一定策略，比如短作业优先，而不是像加锁机制一样，抢先者得到处理器，这样可以避免饥饿现象的发生。 13、相关进程间的制约关系–直接制约直接制约关系：由于进程协作关系造成的制约。 某任务需要通过循环使用read、move、write，3个操作将源磁盘上的数据文件，复制到目标磁盘。一次操作处理一个文件。 如果这3个操作在同一个进程中顺序执行，则当进行IO操作read,write的时候，进程就处在阻塞状态了。 如果这3个操作，分别建立进程，并发执行，则会节约运行时间。 然后，由于并发执行的时候这3个操作执行顺序的随机性，可能会造成重复拷贝文件或者遗漏文件的错误。 1、直接制约与同步关系 对于一组存在直接制约关系的并发进程，为了避免制约关系导致的运行错误，应该让各个进程中代码的运行顺序服从其内在要求，需要建立进程间单向或者相互的依赖关系。 如果一组进程中的每一个进程都要与其他至少一个进程建立了单向或者相互的依赖关系，则把这组进程叫同步进程，这组进程之间建立了同步关系。 2、同步关系的实现–信号量机制 比如有代码段L1对L2有依赖，可以将L2的一次执行看做一种资源的归还，而L1的执行，需要申请使用这种资源。 于是，可以类似用信号量机制来实现同步关系：在L1执行之前先执行原语p，而在L2执行后执行原语v。 与互斥关系不同的是，L1使用资源后并不归还。 3、文件拷贝问题–同步关系的实现 read中的代码段L1 L1将文件src的内容存到缓冲区buf1 然后Move中的代码段L2 L2读出buf1中的内容 然后Move中的代码段L3，再把读出的内容写到buf2 最后write中的代码段L4，再从buf2中读的内容写到别的文件 初始化的时候：先S1=1,S2=0,S3=0,S4=0，P他们刚开始只能S1进行使用处理器资源，其他的都加入到阻塞队列了。 首先Read进程是p(S1),因为S1初始化必须为1，P操作，先是做减1操作，然后再做判断S1是否小于0，如果小于0，先加入到阻塞队列。 为了顺序执行，我们先把S1初始化为1，刚开始S1是1，减1操作后，S1变为0了。此时就是Read进程的L1代码段在使用处理器资源。 当L1的代码段执行完后，需要进行Move了，此时也就是L1已经让出处理器资源，则需要先把S2给唤醒，需要V以下S2。唤醒S2之后S2做加1操作，然后再P(S2)就得到了处理器资源，然后等到P(S2)减1操作，执行完后，等同理。 14、进程同步总结 控制并发进程的互斥，同步关系，保证他们能够正确执行，称作进程同步，实现集成同步的方法叫做进程同步机制。 操作系统需要为程序提供进程同步机制，而当需要处理互斥/同步关系时才决定如何使用这些机制。 用于实现互斥关系，针对间接制约的进程同步关系：加锁机制、信号量机制 用于实现同步关系，针对直接制约的进程同步关系：信号量机制。 除了加锁机制、信号量机制之外，常用的进程同步机制还有：标志位机制，管程机制等。 15、生产者、消费者问题用两个进程Pro和Con分别表示生产者和消费者，用数组B[]表示最大库存为k的仓库， 1234567891011121314151617slot=k;//正信号量表示还有多少个空位item=0;//需要消费的Pro()&#123; 生产货物; p(slot); 查找B中任意空位in; 将货物放入B[in]; v(item);&#125;Con()&#123; p(item); 查找B中任意存有货物的位置out; 取出获取B[out]; v(slot); 消费货物；&#125; 16、从信号量到进程通信 进程的独立性：不同进程无法互相访问代码和数据。 进程需要相互写作必须能够通信（也就是共享受）。 信号量就是一种常见的共享受–低级通信 信号量数据结构的存放位置并不是yoghurt进程的私有空间，而是内核空间，信号量使用前需要向内核申请，用来获得信号量标识。 1、共享存储区通信 共享存储区通信：进程根据需要向内核申请位于内核空间的共享存储区，并将此区域映射到发送进程与接收进程的地址空间，使得发送，接收进程可以通过共享存储区进行通信。 共享存储区中的数据存取没有固定模式，需要注意根据实际情况处理同步/互斥关系。 2、信箱通信 信箱通信：在内核空间中建立专门区域作为信箱，信箱划分有限个信格，每个信格可存放一条信息（信件）。每个信箱可由多个发送进程与接收进程共享 ，发送进程用send操作将信息放入信格中，接收进程用receive操作从信格中取出信息。 与共享存储区通信相比，信箱通信的数据存取模式固定，其实现是一个多生产者消费者的PC问题。 3、消息缓冲通信 消息缓冲通信：内核空间中为每个进程配备一个消息队列MQ，并记录在进程的PCB中。发送进程通过send操作将信息封到一个消息缓冲区中，并追加到接收进程的消息队列中。接收进程通过receive操作收取自身消息队中的信息。 消息缓冲区包含： 发送进程的标识pid 正文大小size 正文data 向下指针next 与信箱通信比较，消息缓冲通信中的消息队列属于进程。消息缓冲通信是一种直接通信，当接收进程没有开启的时候，通信不成功。 4、管道通信 管道通信：通信双方共享一个文件，发送进程向文件中写入数据，接收进程从中读出数据，从而实现通信。共享的文件叫做管道。 管道通信中的共享数据并不是存放在内核空间中，而是存放在管道(文件)中，需要注意维护数据的安全问题。 管道通信中的管道文件，和共享存储区类似，并没有固定数据存取模式，所以需要注意根据实际情况处理互斥同步关系。 管道通信可以使用远程文件作为管道，所以，这样通信方式不局限于单机系统。 17、线程把进程细化成若干个可以独立运行的实体，每一个实体叫做一个线程Thread。 线程和进程的比较 对比项目 进程 线程 独立拥有的资源 处理器控制权和其他资源 处理器控制权 内存访问 不同享 同进程内共享 切换时的开销 大 小 动态性，并发性，处理同步/互斥 二者相同 二者相同 引入线程减小了系统的基本工作单位粒度，目的是： 实现进程内部的并发执行，提供并行程度 减少处理器切换带来的开销 简化进程通信方式 1、线程的控制与类型对线程实施管理，控制的模块称作线程包 根据线程包的实现方式，将线程划分为： 用户级线程：由用户态的线程包来管控，而内核中不识别这种线程 优点：切换时不需要进入到内核 缺点：某一个线程在内核上阻塞的时候，会导致全部进程阻塞。 系统级线程：使用内核提供的线程包来管控 优点：某线程在内核上阻塞时，同进程的其他线程仍然可以运行。 缺点：切换时不需要进入内核。 2、进程的常用细化方法将进程细化为线程需要根据实际应用的需要，主要的细化方法有： 分派/处理模型（Dispatcher/Worker Model） 由一个分派线程Dispatcher负责接收工作需求，然后将工作任务分派给工作线程Worker来完成动作。 队列模型Team Model 将进程细化为具有独立关系的线程，各自完成独立的任务 管道模型Pipeline Model 将完整进程中有序的各执行步骤作为线程，依次顺序运行（流水线） 三、调度调度Scheduling是管理的一种方法，一种决策。目的是通过一种合理的，有效的安排方式，提供资源(比如工作，人力 ，车辆等)的利用率。 操作系统中关注的调度问题有4种： 作业调度(宏观)：考虑何时执行哪些作业(比如Hive的数据抽取脚本) 进程调度(微观)：考虑将哪个就绪进程转换为运行状态 交换调度(中级)：考虑将进程如何在内/外存储器之间交换 设备调度：考虑让哪个进程唤醒使用设备。 1、调度的性能指标评价调度的效果，常使用一组性能指标： 1、周转时间和平均周转时间从处理过程的完成时间来进行评价(批处理系统)，处理过程Ji的周转时间；Ti=Ji的完成时刻-Ji的提交时刻 一批(n个)处理过程的平均周转时间：T=∑Ti/n 2、带权周转时间周转时间并不直接体现某处理过程耗时的合理性。 处理过程Ji的带权周转时间：Twi=Ti/Tri 一批(n个)处理过程的平均带权周转时间：Tw=∑Twi/n 3、响应时间从处理过程的响应速度来进行评价，常用在分时系统，实时系统 处理过程Ji的响应时间： Ri=Ji第一次对用户做出响应的时刻-Ji的提交时刻 4、其他性能指标 公平合理：安排公平，用户满意 资源利用率(并行度)：资源闲置的情况 吞吐量：单位时间完成处理过程的数量 2、作业调度算法1、先到先服务FCFS(First Come First Served)思想：排队 特点： 公平合理 算法简单，容易实现 服务质量欠佳(不利于短作业) 2、短作业优先SJF(Shortest Job First)思想：减少共同等待时间 特点： 实现最小的平均周转时间 吞吐量大 存在饥饿现象 算法简单，但实现困难 3、高响应比优先HRN(Highest Response-ratio Next)思想：通过考虑作业以等待的时间解决SJF的饥饿问题 响应比R=作业等待时间/作业大小 作业等待时间=系统当前时间-作业提交时刻 3、进程调度方式进程调度：在采用并发执行方式的系统中，决定正在运行的进程何时将处理器使用权交给就绪队列中的哪个进程。 进程调度方式指操作系统何时实行调度，也就是处理器使用权何时可能切换，可分为： 非抢占式：除非进程由于自身原因或者外部(非操作系统的)原因交出处理器使用权，否则一直运行直到结束，非剥夺的 抢占式：操作系统定期检查系统状态(定期调度)，按照某种原则主动将进程暂停并将处理器使用权交给下个进程。剥夺式 4、进程调度算法进程调度算法考虑调度进行时的行为，也就是如何判断是否真正发生处理器使用权切换，以及切换时选择哪个进程。 1、先来先服务FCFS 调度时不修改就绪队列，并且总是选择队首。 FCFS基于非抢占调度方式 FCFS进程调度的优缺点类似FCFS的作业调度。（公平，容易实现，服务质量欠佳） 2、时间片轮转RR（Round-Robin） RR基于抢占调度方式，它的实现依赖与计时器中断。 进程每次被选中时分配一个时间片，调度时判断其时间片是否用尽，如果用尽，则将其移到就绪队列末尾，否则不修改就绪队列。并且总是选择队首。 时间片长度为T，如果就绪队列中已经存在了n个进程，则提交新进程的最长响应时间：R=Txn 应用RR算法的关键是根据实际需要确定T的值： T太长则响应时间太长 T太短则进程切换太频繁，系统开销加大。 3、优先级算法Priority 为每个进程赋予一个优先数，调度时总是选择优先数最高的进程。 可基于抢占式调度，也可以基于非抢占式调度： 抢占式调度：可以保证总是执行高优先级进程 非抢占式调度：仅能保证当前一个进程阻塞或者结束时总能选择优先级最高的进程。 优先级调度的关键是进程优先数的配置。按照优先数配置的特点，优先级算法分为： 静态优先级算法：进程优先数在进程创建时确定，并不可改变。 动态优先级算法：进程优先数在创建进程时被赋初始值，运行中可以根据系统状态改变。 通过不同的优先数配置策略，优先级算法可以灵活的体现各种算法思想FCFS，SJF 经验：IO繁忙型的进程应该被赋予高优先级用来提供系统的并行度。 5、实时系统的进程调度实时系统需要对周期性或者非周期性发生的时间作出处理，其对处理正确性和时效性要求十分苛刻。 不同类型的时间对于时效性的要求各异，可表达为以下时间参数： 就绪时限：事件发生时到相应处理进程被创建的时间长度要求。 开始时限：事件发生时到相应处理进程第一次运行的时间长度要求 完成时限：事件发生时到相应处理进程完成的时间长度要求。 处理时间：某些特殊事件要求定期触发处理过程。 5、可调度以及条件 对于一组事件，如果存在某种处理方式，让其中每个事件的时限要求都可以得到满足，就说明这组事件是可调度的Schedulable。否则，如果无论用何种方式，都至少有一个事件无法满足时限要求，则说明这组事件是不可调度的。 可调度的一个必要条件：u&gt;r，u表示系统单位时间内可以处理的事件数（处理能力），r表示单位事件内到达的事件数。 四、死锁一组进程都处于阻塞状态，其中的任一进程阻塞状态的解除都依赖与另一进程的后续操作 。这种系统状态叫做死锁DeadLock。 1、死锁的分类： 资源死锁：由于相互等待临界资源而导致的死锁 通信死锁：由于相互等待通信导致的死锁 控制死锁：由于系统或者用户的特殊控制导致的死锁 2、死锁的特点 偶发性：由于特殊的并发执行顺序导致，是小概率事件 非消耗：死锁的相关进程处于阻塞状态，不占用处理器 程序无关：并非个别程序设计的错误，而是系统运行的错误。 3、死锁的原因资源死锁的根本原因是可用资源的数量小于所需资源的数量。 但不可能靠无限制的增加资源数量来根本性的解决死锁问题。 死锁的发生的必要条件： 互斥条件：死锁的相关进程都是互斥的 不剥夺条件：当进程申请资源时不能剥夺其他进程正在使用的资源 请求与保持(部分分配)条件：进程在申请资源不成功的时候仍然保持已经申请的资源。 环路等待条件：进程相互等待关系形成环路。 4、死锁的预防通过在资源分配的时候采取一系列的限制措施，来破坏4个必要条件之一，就可以避免死锁的发生。 互斥条件的破坏 方法：不允许申请临界资源 缺点：临界资源需要转化为非临界资源才能被申请和使用，大部分情况下无法实现。 不剥夺条件的破坏 方法：允许进程申请资源时剥夺其他进程正在使用的资源 缺点：被剥夺资源的进程不可预期的打断，系统需要处理回退。缺乏公平性，反复剥夺与被剥夺 请求与保持条件的破坏 方法1：资源暂时归还 进程申请资源不成功时，暂时归还已经分配的资源。 缺点：系统需要处理回退，反复申请与归还。 方法2：静态分配 进程需要一次性申请完所有可能使用的资源 缺点：难以预知需要使用的资源，降低资源利用率。 环路等待条件的破坏 方法1：单请求方式 资源申请不允许嵌套，但可以一次申请多个，归还旧的资源后方可以申请新资源。 缺点：同时需要多个资源时只能实行小范围内的静态分配。 方法2：按序分配 对资源编号，进程只允许申请比当前已拥有资源编号更大的资源。 缺点：如果使用大编号资源的段内需要使用小编号资源，则只能实行小范围内的静态分配。而且，编号管理也困难。 5、死锁的经典问题哲学家用餐问题 6、死锁的避免进程可以自由的动态的，申请资源，但是分配资源的时候对可能发生的死锁，进行预测，来决定是否立即分配。 死锁的避免具有以下特点： 原理上是通过预测潜在的死锁 不影响程序设计 临界资源分配并非空闲让进 7、银行家算法 初始化，计算资源可用量Avaiable和当前需求矩阵Need， Available=Total-∑Used Need=Max-Used 合法检查，检查资源申请与需求矩阵的相容性，未通过则报错，Request&lt;=Need 资源检查，检查请求的资源是否可用，未通过则阻塞进程P。Request&lt;=Avaiable 预分配，嘉定Request得到满足 Available=Available-Request Used=Used+Request Need=Need-Request 安全状态检查，检查预分配后的状态是否为安全状态。 某系统有4类资源A,B,C,D，数量分别是8,10,9,12。当前有5个进程P1，P2，P3，P4，P5。最大需求矩阵Max和当前分配矩阵Used如下：$$Max = \\left[ \\matrix{ 4 &amp; 6 &amp; 3 &amp; 8\\ 3 &amp; 3 &amp; 5 &amp; 2\\ 6 &amp; 6 &amp; 0 &amp; 9\\ 3 &amp; 4 &amp; 8 &amp; 5\\ 4 &amp; 3 &amp; 2 &amp; 2 } \\right]$$ $$Used = \\left[ \\matrix{ 1 &amp; 1 &amp; 0 &amp; 2\\ 2 &amp; 2 &amp; 4 &amp; 0\\ 0 &amp; 5 &amp; 0 &amp; 1\\ 1 &amp; 1 &amp; 1 &amp; 5\\ 2 &amp; 0 &amp; 2 &amp; 2 } \\right]$$ 在当前状态下，如果进程P1申请request=(1,0,1,0)，系统能否分配？ 资源总量(8,10,9,12) 进程 Max Used Need Avaiable Finished A B C D A B C D A B C D 1 1 1 2 p1 4 6 3 8 1 1 0 2 p2 3 3 5 2 2 2 4 0 p3 6 6 0 9 0 5 0 1 p4 3 4 8 5 1 1 1 5 p5 4 3 2 2 2 0 2 2 在当前状态下，如果进程P3申请request=(1,0,0,1),系统能否分配？ 五、存储管理1、存储器的类型 外存储器、磁带，离线存储，容量大，速度慢 外存储器，磁盘，光盘等，主板外 cache主存储器，主板类（存储器管理） 寄存器cache，cpu内，速度块，容量小 2、程序的编码与装入 程序（高级语言） 123456int a=10;main()&#123; int b=a; cout&lt;&lt;b&lt;&lt;a; cout&lt;\"hello\";&#125; 编译链接 程序（机器码） 123450~1 102~8 hello.9 ???? (将[0~1]压栈)10~13 ???? (cout&lt;&lt;[esp]&lt;&lt;[0~1];)14~17 ???? 装入(count&lt;&lt;[2~8];) 运行时内存 数据段：全局变量和敞亮 代码段：指令存放处 堆栈：局部变量存放处（由指令动态操作） 3、逻辑地址与物理地址​ 程序在运行的时候被装入到内存中，其中的指令与数据的位置用地址值表示，地址值的变现形式可分为： 逻辑（虚拟）地址：指令或者数据在程序中的编号（位置） 物理地址：指令或者数据内存中的位置 物理地址 逻辑地址 程序（机器码） 10000~10001 0~1 10 10002~10008 2~8 hello 10009 9 (将[0~1]压栈；) 10010~10013 10~13 （cout&lt;&lt;[esp]&lt;&lt;[0~1];） 10014~10017 14~17 (cout&lt;&lt;[2~8];) 4、重定位（地址转换）​ 若程序中的变量与指令用逻辑地址加以引用，则为了在运行的时候正确访问这些变量或者指令，需要进行重定位（或者叫地址转换），来确定实际的物理地址。 ​ 根据地址转换操作的时机，重定位分为： 静态重定位：操作系统在装入程序时候修改其中所引用的逻辑地址值为物理地址值–由操作系统实现。 动态重定位：在运行过程中需要做访问操作的时候才进行地址转换–由cpu的存储管理单元MMU自动实现，操作系统只登记MMU进行重定位时需要的相关信息。 个人理解，Java的反射机制，可以在运行的时候，获取Class信息，想必底层实现原理就是利用的动态重定位吧。 5、静态重定位与动态重定位的特点对比： 静态重定位 动态重定位 硬件支持 不需要 需要 操作系统的工作 装入时修改程序 登记MMU所需要信息 程序装入 被修改 直接装入 程序运行时移动 困难 容易 程序的不连续装入 困难 容易 其他技术支持（动态链接、虚拟存储等） 难以实现 容易实现 6、存储器管理的关键问题​ 存储器管理主要是对用户空间进行管理，目的是为了提供主存储器的利用率，方便用户对主存储器的使用。 ​ 存储器管理的关键问题： 1、存储空间的分配与回收 设计合理的数据结构以记录存储空间的分配情况 设计合理的算法来提高存储器的利用率（装入尽可能多的程序） 2、重定位方式的确定 决定采用静态重定位或者动态重定位，并进行相应的工作 3、实现保护与共享 存储空间保护使得各进程不能访问彼此的存储空间，主要方法有： 利用出其里的特权级，防止跨级访问 利用MMU的界限寄存器规则，防止同级访问 操作系统设置和检查存储区域的保护键与当前进程的键以及操作是否匹配 存储空间共享： 提供进程间共享数据的途径 4、实现虚拟存储技术7、存储器管理的方法1、从程序的角度分为： 非分段管理：将进程的数据、代码、堆栈作为一个完整的逻辑空间 分段管理：将进程的数据、代码、堆栈、各自作为独立的逻辑空间 2、从内存分配的角度分为： 分区管理：一个逻辑空间对应物理空间中一个分区，连续分配 分页管理：一个逻辑空间对应物理空间中多个分页，可不连续分配。 段页式管理：比较先进复杂的综合方法。 8、单一连续区存储管理​ 操作系统占用内核空间之外，将整个用户空间看做一个内存区域，一次只能装入一道用户程序，只有用户区中的程序运行完毕方可以装入下一道程序。 也就是说，一个内存，有系统区，还有用户区。系统区，存储一个操作系统信息。 常用静态重定位 管理简单 不需要复杂硬件支持 只允许单任务 为了实现单一连续区存储管理，或者其他任何存储管理的方法，操作系统采用的请求表来记录请求装入内存的程序信息。 程序标识 需要的空间长度（虚拟地址空间大小） A 50K B 90K C 130K D 10K E 165K 9、固定分区存储管理​ 操作系统占用内核空间之外，将整个用户空间划分为若干个固定大小的区域（大小可不相等）。多道程序分别装入不同的分区内，一道程序只能装入一个分区，而一个分区也只能分配给一道程序。 ​ 为了实现固定分区存储管理，操作系统必须记录各分区的位置和使用情况，采用关键数据结构–分区说明表（DPT） 区号 长度 起始地址 状态 1 75K 32K 0 2 30K 107K 0 3 140K 137K 0 4 11K 277K 0 ​ 固定分区存储管理一般采用静态重定位，配合界限寄存器法防止各进程彼此访问存储区域（存储保护）。 ​ 利用MMU的界限寄存器规则，防止同级间房屋内，操作系统设置和检查存储区域的保护建与当前进程的键是否匹配。 ​ 固定分区存储管理的特点： 支持多道程序设计 并发执行进程数受到分区数限制 程序可用空间受分区大小限制 存储区存在“碎片” 长作业优先、输入队列法 10、可变分区存储管理​ 在程序装入时选择一个空闲区域并在其中动态创建一个分区来装入程序。 ​ 为了实现可变分区存储管理，操作系统需要记录空闲内存区域的位置，可采用以下两种数据结构： 1、可用表类似分区说明表DPT，但是其中的记录是变化的，并且状态1表示空闲，而状态0表示记录无效（预留着存储位置） 起始地址 长度 状态 152K 10K 1 172K 116K 1 0 0 0 缺点：可用表的长度受到了限制，从而导致表示的空闲区个数受到限制，影响并发的进程个数 2、空闲区链表用链表的形式来记录空闲区 ps：怪不得面试的时候，让手写LRU算法，而LRU算法也是用链表实现的。 12345struct FreeNode&#123; long start;//分区的起始地址 long length;//分区的长度 struct FreeNode *next;//向下指针&#125; 3、可变分区存储管理空间分配算法当程序请求装入的时候，进行空间分配的算法： 最先适应法FF：依次查找空闲区，选择收个足够大的空闲区装入程序。 最佳适应法BF：在所有足够装入程序的空闲区中选择最小者 最坏适应法WF就：在所有足够装入程序的空闲区中选择最大者 4、空间分配算法优缺点：三种算法的优势和缺点： 最先适应法FF：分配速度最快，但是堆内存使用没有规划。 最佳适应法BF：找到的空闲区与请求最匹配，对大程序的装入有充分的准备，但是容易产生外碎片。 最坏适应法WF：不容易产生外碎片，但是容易造成大程序装入的失败。","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"Spark的学习","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、Spark的学习/","text":"一、Spark Core的学习1、下载并安装Spark下载链接 123456789101112131415161718192021#安装spark相关命令cd /usrsudo mkdir sparkcd ~/下载/软件安装包sudo mv spark-2.4.4-bin-hadoop2.7.tgz /usr/spark/cd /usr/spark/sudo tar -zxvf spark-2.4.4-bin-hadoop2.7.tgzcd spark-2.4.4-bin-hadoop2.7/#配置spark相关文件,切换到conf目录cd conf/#复制一个模板sh文件，设置spark的cp spark-env.sh.template spark-env.sh#配置spark需要的jdk环境，以及使用的内存，核心数export JAVA_HOME=/usr/java/jdk1.8.0_201export SPARK_MASTER_IP=SparkMasterexport SPARK_WORKER_MEMORY=2gexport SPARK_WORKER_CORES=2export SPARK_WORKER_INSTANCES=1cp slaves.template slaves #配置spark的slavevim slaves 2、首先要有jdk，scala1234#修改当前用户的环境变量文件vim ~/.bashrc#保存source ~/.bashrc 3、操作spark并启动1234567891011121314151617cd /sbin#如果发现报错，拒绝连接本地，则说明没有安装ssh#启动spark的简单集群./start-all.sh#发现还是一样，拒绝连接本地ssh localhost#发现只有ssh-agent，而没有sshdps -e|grep ssh#安装sshsudo apt updatesudo apt upgradesudo apt install openssh-serverps -e|grep ssh#重新启动./start-all.sh#打开浏览器访问lhttp://victor:8081/ 启动spark-shell 12345cd bin/#python下的spark shellpyspark#scala下的spark shellspark-shell 1、简介 Spark流是对于Spark核心API的扩展，从而支持对于实时数据流的可扩展，高吞吐量和容错性流处理。数据可以由多个源取的，比如Kafka，Flume，ZeroMQ，或者TCP接口等。可以同时使用比如map，reduce，join和window这样的高层接口描述的复杂算法进行处理。最终，处理过得数据可以保存到HDFS，数据库等。 在内部，Spark Streaming接收到实时数据流同时将其划分为分批，也就是微批处理。这些数据的分批将会被Spark的引擎所处理而生成同样按批次形式的最终流。 Spark Streaming提供了被成为离散化或者DStream的高层抽象，这个高层抽象用于表示数据的连续流。 创建DStream的2种方式： 由Kafka，Flume(一个日志收集中间件)，取的数据作为输入流 在其他DStream进行的高层操作。 在内部，DStream被表达为RDDs的一个序列。 2、Spark的组件介绍2.1、Spark Core​ 实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块。Spark Core中包含了对RDD(弹性分布式数据集)的API定义。 ​ RDD表示分布在多个计算节点上可以并行操作的元素集合，是Spark的主要编程抽象。Spark Core提供了创建和操作这些集合的多个API。 2.2、SparkSQL​ SparkSQL是Spark用来操作结构化数据的程序包。通过SparkSQL，可以使用SQL或者Hive的HQL来查询数据。SparkSQL支持多种数据源，比如Hive表，JSON等。 ​ SparkSQL除了提供一个SQL接口，还支持将SQL和传统的RDD编程的数据相结合的操作。 2.3、Spark Streaming​ Spark Streaming是Spark提供对实时数据进行流式计算的组件。比如生产环境的网页服务器日志，或者用户提交的状态更新组成的消息队列，都是数据流。Spark Streaming提供了用来操作数据流的API。与Spark Core中的RDD API高度对应。不论是操作内存或者硬盘的数据，还是操作实时的数据流，都可以应对自如。Spark Streaming支持与Spark Core同级别的容错性、吞吐量以及可伸缩性。 2.4、MLib​ Spark中的一个提供常见机器学习ML的库。提供了很多机器学习算法，包括分类、回归、聚类、协同过滤等。还提供了模型评估、数据导入等额外的支持功能功能。MLib还提供了一些更底层的机器学原语，包含一个通用的梯度下降优化算法。 2.5、GraphX​ GraphX是用来操作图的程序库，可以进行并行的图计算。与Spark Streaming和SparkSQL类似。GraphX也扩展了Spark Core的RDD API，用来创建一个顶点和边都包含任艺术型的有向图。 ​ 个人理解：有点类似TensorFlow的TensorBoard。 1.6、集群管理器​ 包括Hadoop YARN，Apache Mesos，以及Spark自带的一个简易调度器，独立调度器。这样可以高效的在一个计算节点到数千个计算节点之前伸缩设计算。如果单独安装Spark，没有任何集群管理的机器，则有子代的独立调度器。或者在装有Hadoop YARN或者Mesos的集群上安装Spark。 1.7、总结​ Spark不仅可以读取HDFS上的文件作为数据集，还支持本地文件，Amazon S3，Cassandra，Hive，HBase等数据集。总之，Hadoop不是Spark的必须必备的。 3、Spark的使用3.1、在Java中使用Spark​ 连接Spark的过程在各个语言中不一样。在Java和Scala中，只需要在pom.xml中添加spark-core的maven依赖。 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt; 3.2、初始化SparkContext​ 完成应用项目与Spark的连接，需要在程序中导入Spark的包，并创建SparkContext上下文。 1234567891011121314151617181920import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaSparkContext;public void test3()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext javaSparkContext = new JavaSparkContext(sparkConf); //读取文件里的数据作为输入数据 JavaRDD&lt;String&gt; rdd = javaSparkContext.textFile(\"/inputFile\"); //切分为单词，以空格为分割 JavaRDD&lt;String&gt; words = rdd.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //转换为kv并计数。把a:1,b:1,c:1,b:1转为a:1,b:2,c:1如此格式。 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair((PairFunction&lt;String, String, Integer&gt;) s -&gt; new Tuple2(s, 1)) .reduceByKey((Function2&lt;Integer, Integer, Integer&gt;) (x, y) -&gt; x + y); //将统计出来的单词总数存入一个文本文件。 counts.saveAsTextFile(\"/outputFile\"); //关闭spark javaSparkContext.stop(); System.out.println(javaSparkContext.version()); &#125; setMaster()：传递集群的URL，告诉Spark如何连接到集群上。 setAppName():当你连接到一个集群时，这个值可以帮你在集群管理器cluster manager的用户界面中找到你的应用。 3.3、总结​ Spark的核心编程，通过一个驱动器SparkConf创建一个SparkContext和一系列的RDD。然后进行并行操作。 4、RDD的使用​ Spark对数据的核心抽象：弹性分布式数据集(Resilient Distributed Dataset)。RDD本质就是分布式的元素集合。在Spark中，对数据的所有操作就是创建RDD，转化已有的RDD和调用RDD进行求值。Spark会自动将RDD中的数据分发到集群，操作并行化执行。 ​ RDD是Spark的核心 4.1、创建RDD​ RDD是一个不可变的分布式对象集合。每个RDD都被分成多个分区，这些分区运行在集群中不同的节点上。 ​ 可以使用2种方法创建RDD： 读取外部数据集 比如使用SparkContext.textFile()来读取文本文件作为一个字符串RDD。 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * spark有2种创建RDD的方式：1、读取外部数据集 2、在驱动器程序中对一个集合进行并行化操作。 */ package spark;import org.apache.spark.SparkConf;import org.apache.spark.api.java.JavaPairRDD;import org.apache.spark.api.java.JavaRDD;import org.apache.spark.api.java.JavaSparkContext;import scala.Tuple2;import java.util.Arrays;import java.util.List;import java.util.regex.Pattern;/** * @Description: * @Author: VictorDan * @Date: 19-11-6 下午2:31 * @Version: 1.0 */public class Spark &#123; private static final Pattern SPACE = Pattern.compile(\" \"); public static void main(String[] args) &#123; SparkConf conf = new SparkConf().setMaster(\"local\").setAppName(\"wc\"); JavaSparkContext context = new JavaSparkContext(conf); JavaRDD&lt;String&gt; lines = context.textFile(\"/home/victor/桌面/file.txt\"); JavaRDD&lt;String&gt; words = lines.flatMap(s -&gt; Arrays.asList(SPACE.split(s)).iterator()); JavaPairRDD&lt;String, Integer&gt; ones = words.mapToPair(s -&gt; new Tuple2&lt;&gt;(s, 1)); JavaPairRDD&lt;String, Integer&gt; counts = ones.reduceByKey((i1, i2) -&gt; i1 + i2); List&lt;Tuple2&lt;String, Integer&gt;&gt; output = counts.collect(); for (Tuple2&lt;?, ?&gt; tuple : output) &#123; System.out.println(tuple._1() + \": \" + tuple._2()); &#125; //spark去重操作 JavaPairRDD&lt;String, Integer&gt; distinct = counts.distinct(); System.out.println(distinct); //spark合并 List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = distinct.collect(); collect.forEach(System.out::println); &#125;&#125; 在驱动程序里分发驱动器程序中的对象集合(list,set) 12345678910111213141516171819202122@Testpublic void parallizeDataCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); List&lt;String&gt; list=new ArrayList&lt;&gt;(); list.add(\"abc\"); list.add(\"i like apple\"); list.add(\"hello world\"); JavaRDD&lt;String&gt; rdd = context.parallelize(list); JavaRDD&lt;String&gt; cache = rdd.cache(); JavaRDD&lt;String&gt; words = cache.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //转换为kv并计数。把a:1,b:1,c:1,b:1转为a:1,b:2,c:1如此格式。 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair((PairFunction&lt;String, String, Integer&gt;) s -&gt; new Tuple2(s, 1)) .reduceByKey((Function2&lt;Integer, Integer, Integer&gt;) (x, y) -&gt; x + y); //spark去重操作 JavaPairRDD&lt;String, Integer&gt; distinct = counts.distinct(); //spark合并 List&lt;Tuple2&lt;String, Integer&gt;&gt; collect = distinct.collect(); collect.forEach(System.out::println);&#125; 4.2、操作RDD创建RDD后，支持2种类型操作： transformation转化操作 转化操作会将一个RDD生成一个新的RDD。filter() action行动操作 行动操作first()。Spark会扫描文件直到找到第一个匹配的行为为止，并不一定要读取整个文件 4.2.1、转化操作​ RDD的转化操作是返回新的RDD的操作。转化操作是惰性的，也就是不会改变已有的rdd中的数据。只有在行动操作的时候，rdd才会被计算。 1234//读取文件里的数据作为输入数据,创建rdd JavaRDD&lt;String&gt; rdd = javaSparkContext.textFile(\"/home/victor/桌面/inputFile.txt\"); //转化操作，把创建的rdd，通过filter过滤掉含有abc的，成为新的rdd JavaRDD&lt;String&gt; filter = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"abc\")); filter()操作不会改变rdd中的数据。实际上filter操作会返回一个全新的RDD。rdd在后面的程序中还可以继续使用。 123456//转化操作，把创建的rdd，通过filter过滤掉含有abc的，成为新的rdd JavaRDD&lt;String&gt; filter = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"abc\")); JavaRDD&lt;String&gt; errorRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"error\")); //union操作 JavaRDD&lt;String&gt; union = filter.union(errorRDD); union.cache(); union()操作与filter()操作不同在于是操作两个RDD。转化操作可以操作任意数量的输入RDD。 总结：转化操作，可以从已有的RDD中派生出新的RDD，Spark会通过lineage graph谱系图来记录这些不同RDD之间的依赖关系。 4.2.2、行动操作​ 行动操作是第二种类型的RDD操作，会把最终的求得结果返回给程序，或者写入到外部存储系统里。 ​ 比如你的collect()函数操作，只有当你的整个数据集在单机的内存中放得下的时候，才可以使用collect()，不能在大规模数据集上使用。 ​ 一般我们都是把数据写到HDFS或者Amazon S3的分布式文件系统里。或者也可以通过调用saveAsTextFile()，saveAsSequenceFile()等类似外部文件里。 ​ 注意：每次调用一个新的行动操作，整个RDD都会从头开始计算。一般都是将中间结果持久化。 1234567//切分为单词，以空格为分割 JavaRDD&lt;String&gt; words = persist.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //转换为kv并计数。把a:1,b:1,c:1,b:1转为a:1,b:2,c:1如此格式。 JavaPairRDD&lt;String, Integer&gt; counts = words.mapToPair((PairFunction&lt;String, String, Integer&gt;) s -&gt; new Tuple2(s, 1)) .reduceByKey((Function2&lt;Integer, Integer, Integer&gt;) (x, y) -&gt; x + y); //将统计出来的单词总数存入一个文本文件。 counts.saveAsTextFile(\"/home/victor/桌面/outputFile.txt\"); 4.2.3、惰性求值​ RDD的转化操作都是惰性求值，也就是在Spark调用行动操作之前是不是从头开始计算的。 ​ 可以把RDD看做是通过转化操作构建出来的，记录着特定数据的指令列表。把数据读取到RDD的操作也是惰性的。也就是调用sc.textFile()的时候，其实数据并没有读取进来，而是在必要的时候才会读取，而且读取数据的操作可能会执行多次。 ​ Spark使用惰性求职，可以把一些操作合并到一块，从而减少计算数据的步骤。 5、向Spark传递函数5.1、Java中传递方式​ 在java中，函数需要作为实现了Spark的org.apache.spark.api.java.function包中的任一函数接口的对象来传递。 123456789//方式1：使用匿名内部类的方式进行函数传递JavaRDD&lt;String&gt; errorRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"error\"));//方式2：通过创建一个具体类，实现接口的方式进行函数传递class ContainsError implements Function&lt;String,Boolean&gt;&#123; @Override public Boolean call(String s) throws Exception &#123; return s.contains(\"error\"); &#125; &#125; 一般大型程序，单独组织类能比较好。而一些简单的直接使用匿名内部类的方式。 6、常见的转化操作和行动操作6.1、转化操作​ 两个最常见的转化操作map()和filter()。 map()：接收一个函数，把这个函数用在RDD中的每个元素，将函数的返回结果作为结果RDD中对应元素的值。 123456789101112@Test public void mapCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;Integer&gt; rdd = context.parallelize(Arrays.asList(1,3,2,5)); //计算RDD中各个数值的平方 JavaRDD&lt;Integer&gt; map = rdd.map((Function&lt;Integer, Integer&gt;) x -&gt; x * x); System.out.println(map.collect()); &#125; filter()：接收一个函数，将RDD中满足函数的元素放入到新的RDD中 flatMap()：和map()类似，有时候我们希望对每个输入元素生成多个输出元素。 12345678910111213@Test public void flatMapCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;String&gt; rdd = context.parallelize(Arrays.asList(\"hello world\",\"i love you\")); //用flatMap()将数据切分为单词 JavaRDD&lt;String&gt; word = rdd.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); String first = word.first(); System.out.println(first);//hello &#125; 集合操作 RDD本身不是严格意义上的集合，但是也支持许多集合操作。比如合并，相交等。 只要唯一的元素distinct() RDD.distinct()操作的开销很大，它需要将所有的数据通过网络进行混洗shuffle，来确保每个元素只有一份。 union,合并操作 subtract:移除一些数据。 intersection:求2个RDD共同的元素 distinct:去重 sample：对RDD采用 1234567891011121314151617181920212223@Test public void flatMapCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;String&gt; rdd = context.parallelize(Arrays.asList(\"hello world\",\"i love you\",\"error\")); //去重 JavaRDD&lt;String&gt; distinct = rdd.distinct(); JavaRDD&lt;String&gt; helloRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"hello\")); JavaRDD&lt;String&gt; errorRDD = rdd.filter((Function&lt;String, Boolean&gt;) s -&gt; s.contains(\"error\")); //用flatMap()将数据切分为单词 JavaRDD&lt;String&gt; word = rdd.flatMap((FlatMapFunction&lt;String, String&gt;) s -&gt; (Iterator&lt;String&gt;) Arrays.asList(s.split(\" \"))); //移除errorRDD成为新的RDD JavaRDD&lt;String&gt; subtract = rdd.subtract(errorRDD); //求errorRDD与helloRDD的共同的元素 JavaRDD&lt;String&gt; intersection = helloRDD.intersection(errorRDD); //对RDD进行采样，以及是否替换 JavaRDD&lt;String&gt; sample = rdd.sample(false, 0.5); String first = word.first(); System.out.println(first);//hello &#125; 6.2、行动操作​ 最常见的行动操作就是reduce()，接收一个函数作为参数，操作2个RDD的元素类型的数据返回一个同样类型的新元素。 1234567891011121314151617181920212223242526272829/** * 行动操作 */ @Test public void actionCreateRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); //读取list JavaRDD&lt;Integer&gt; rdd = context.parallelize(Arrays.asList(1,3,2,5)); //计算RDD中各个数值的平方 JavaRDD&lt;Integer&gt; map = rdd.map((Function&lt;Integer, Integer&gt;) x -&gt; x * x); //reduce Integer reduce = map.reduce((Function2&lt;Integer, Integer, Integer&gt;) (a, b) -&gt; a + b); //返回RDD中所有元素 List&lt;Integer&gt; collect = rdd.collect(); //RDD中元素的个数 long count = rdd.count(); //各个元素在RDD中出现的次数 Map&lt;Integer, Long&gt; integerLongMap = rdd.countByValue(); //从RDD中返回2个元素 List&lt;Integer&gt; take = rdd.take(2); //从RDD中返回最前面的2个元素 List&lt;Integer&gt; top = rdd.top(2); //从RDD中按照提供的顺序返回最前面的2个元素 List&lt;Integer&gt; order = rdd.takeOrdered(2); System.out.println(map.collect()); &#125; 6.3、在不同RDD类型间转换12345678910111213141516171819202122public void diffTypeRDD()&#123; //连接spark集群 SparkConf sparkConf = new SparkConf().setMaster(\"local\").setAppName(\"wordCount\"); //创建上下文 JavaSparkContext context = new JavaSparkContext(sparkConf); JavaRDD&lt;Integer&gt; rdd = context.parallelize(Arrays.asList(1, 3, 2, 5)); //创建DoubleRDD JavaDoubleRDD doubleRDD = rdd.mapToDouble((DoubleFunction&lt;Integer&gt;) x -&gt; x*x); //mean只能用在数值上 Double mean = doubleRDD.mean(); //mean只能用在数值上 Double variance = doubleRDD.variance(); //pairRDD的使用 JavaPairRDD&lt;Integer, Integer&gt; pairRDD = rdd.flatMapToPair((PairFlatMapFunction&lt;Integer, Integer, Integer&gt;) x -&gt; &#123; ArrayList&lt;Tuple2&lt;Integer, Integer&gt;&gt; tpLists = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; i++) &#123; Tuple2 tp = new Tuple2&lt;&gt;(x, i); tpLists.add(tp); &#125; return tpLists.iterator(); &#125;); &#125; 6.4、总结​ 学习了RDD运行模型和RDD的许多操作，已经学习并了解了Spark Core。我们在进行并行聚合，分组的时候，常常都是以kv的形式RDD。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"G1垃圾回收","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、G1垃圾回收器介绍/","text":"一、G1垃圾回收器介绍为了解决CMS算法产生空间碎片和其他一系列的问题缺陷，HotSpot虚拟机团队提出了另外一种垃圾回收策略，G1（Garbage First）算法。通过参数-XX:+UserG1GC来启用。这个算法在JDK7被正式推出。 G1垃圾回收算法主要应用在多CPU大内存的服务中，在满足高吞吐量的同事，尽可能的满足垃圾回收时的暂停时间，G1的设计应用场景： 垃圾收集线程和应用线程并发执行，和CMS一样。 空闲内存亚索时避免冗长的暂停时间 应用需要更多可预测的GC暂停时间 不希望牺牲太多的吞吐性能 不需要很大的java堆 1、堆内存结构：1、以往的垃圾回收算法，比如CMS，堆内存结构如下：年轻代/新生代：eden space+2个survivor 老年代：old space 持久代：1.8之前的perm space 元空间：1.8之后的metaspace(元空间) 上面这些space必须是地址连续的空间。 2、在G1算法中，采用了另一种完全不同的方式组织堆内存堆内存被划分成多个大小相等的内存块Region，每个Region是逻辑连续的一段内存。 每个Region被标记了E,S,O,H，说明每个Region在运行时都充当了一种角色，其中H是以往算法中没有的，它代表Humongous，这表示这些Region存储的是巨型对象。当新建对象大小超过Region大小一半的时候，直接在新的一个或者多个连续Region中分配，并标记为H。 3、Region堆内存中一个Region的大小可以通过-XX:G1HeapRegionSize参数指定，大小区间只能是1M,2M,4M,8M,16M,32M，总之是2的幂次方，如果G1HeapRegionSize为默认值，则在堆初始化时计算Region的实际大小。 默认把堆内存按照2048份均分，最后得到一个合理的大小。 2、GC模式G1中提供了3种模式垃圾回收模式，young GC,mixed GC和full GC。在不同的条件下被触发 1、young GC发生在年轻代的GC算法，一般对象（除了巨型对象），都是在eden region中分配内存，当所有的eden region被消耗殆尽无法再申请内存的时候，就会触发一次young GC，这种触发机制和之前的young GC差不多，执行玩一次young GC，活跃对象会被拷贝到survivor region或者晋升到old region中，空闲的region会被放入空闲列表中，等待下次被使用。 参数 含义 -XX:MaxGCPauseMillis 设置G1收集过程目标时间，默认200ms -XX:G1NewSizePercent 新生代最小值，默认值5% -XX:G1MaxNewSizePercent 新生代最大值，默认值60% 2、mixed GC当越来越多的对象晋升到老年代old region时，为了避免堆内存被消耗殆尽，虚拟机会触发一个混合的垃圾收集器，也就是mixed GC；这个算法不是一个old GC，除了回收整个young region，还会回收一部分old region。注：只是一部分老年代，而不是全部老年代，可以选择哪些old region进行收集，从而可以对垃圾回收的耗时时间进行控制。 mixed GC什么时候被触发？ 有点类似cms的触发机制 ，如果添加了以下参数： 12-XX:CMSInitiatingOccupancyFraction=80-XX:+UserCMSInitiatingOccupancyOnly 当CMS的老年代的使用率达到80%时，就会触发一次cms gc。 相对的，mixed gc中也有一个阈值参数： 1-XX:InitiatingHeapOccupancyPercent 当老年代大小占整个堆大小百分比达到该阈值时，会触发一次mixed gc。 3、mixed GC执行过程 initial remark：初始标记过程，整个过程STW（Stop-The-World，在执行垃圾回收算法的时候，Java应用程序的其他所有线程都被挂起，Java中一种全局暂停现象，全局停顿，所有Java代码停止，native代码可以执行，但是不饿能够和JVM交互）标记了从GC Root可达的对象 concurrent marking：并发标记过程，整个过程gc collector线程与应用线程可以并行执行，标记出GC Root可达对象衍生出来的存活的对象，并收集各个Region的存活对象信息。 remark：最终标记过程，整个过程STW，标记出哪些在并发标记过程中遗漏的，或者内部引用发生变化的对象。 clean up：垃圾清除过程，如果发现一个Region中没有存活对象，则把该Regioon加入到空闲列表中。 4、full GC如果对象内存分配速度过快，mixed GC来不及回收，导致老年代old Region被填满，就会触发一次full gc。 G1的full GC算法就是单线程的执行serial old gc，会导致异常长时间的暂停时间，需要进行不断的调优，尽可能的避免full GC。","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"分布式锁的使用","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、3种分布式锁实现原理/","text":"一、分布式锁1、为什么要使用分布式锁​ 如果需要对一个共享变量进行多线程同步访问的时候，可以使用java多线程进行运行。 ​ 如果是单机应用，也就是所有的请求都会分配到当前服务器的JVM内部，然后映射为操作系统的线程进行处理，而这个共享变量只是在这个JVM内部的一块内存空间。 ​ 后续业务发展，需要做集群，一个应用需要部署好几台机器然后做负载均衡。 一个共享变量A，存在JVM1，JVM2，JVM3，这3个JVM内存中，这个变量A主要体现是在一个类中的一个成员变量，是一个有状态的对象。 比如UserController中的一个int类型的成员变量。如果不加任何控制的话，变量A同时都会在JVM分配一块内存，3个请求发过来，同时对这个变量操作，显然结果是不对的。即使不是同时发过来，3个请求分别操作3个不同JVM内存区域的数据，变量A之间不共享的话，也不具有可见性的话，处理结果也是不对的。 2、解决问题​ 为了保证一个方法或者属性在高并发情况下的同一时间只能被同一个线程运行，在传统单体应用单机部署的情况下，可以使用Java并发处理相关的API（比如ReentrantLock，Synchronized）进行互斥控制。在单机环境中，Java中提供了很多并发处理的API。但是随着业务发展的需要，原来的单机部署的系统被演化为分布式集群系统后，由于分布式系统多线程，多进程并且分布在不同机器上，这将使得原单机部署情况下的并发控制锁策略失效，单纯的Java API并不能呢提供分布式锁的能力。为了解决这个问题就需要一种跨JVM的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题。 3、分布式锁具备的条件 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行。 高可用的获取锁与释放锁 高性能的获取锁与释放锁 具备可重入特性 具备锁失效机制，防止死锁 具备非阻塞锁特性，也就是没有获取到锁将直接返回获取锁失败。 4、分布式锁的3种实现方式​ 目前大型网站应用都是分布式部署，分布式场景中的数据一致性问题一直是一个重要的问题。出现了CAP定理。任何一个分布式系统都无法同时满足一致性(Consistency)，可用性(Availablility)，分区容忍性(Partition tolerance)，只能同时满足2个。 ​ 目前在绝大多数场景中，都是需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证最终一致性就行。也就是只要这个最终时间是在用户可以接受的范围内就行。 ​ 在很多场景中，我们为了保证数据的最终一致性，需要很多技术支持。比如分布式事务，分布式锁。需要保证一个方法在同一个时间内只能被同一个线程执行。 基于数据库实现分布式锁 基于缓存Redis等，实现分布式锁 基于Zookeeper实现分布式锁 1、基于数据库实现排它锁(互斥锁)1.1、解决方法1：使用唯一索引约束。12345678DROP TABLE IF EXISTS `method_lock`;CREATE TABLE `method_lock` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` varchar(64) NOT NULL COMMENT '锁定的方法名', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='锁定中的方法'; 12--获取锁insert into method_lock(method_name,desc) values('xxxService','methodName';) 这种解决方法，对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功。 1.2、解决方法2：先获取锁的信息，然后更新状态。12345678910DROP TABLE IF EXISTS `method_lock`;CREATE TABLE `method_lock` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT COMMENT '主键', `method_name` varchar(64) NOT NULL COMMENT '锁定的方法名', `state` tinyint NOT NULL COMMENT '1:未分配；2：已分配', `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, `version` int NOT NULL COMMENT '版本号', `PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name`) USING BTREE) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='锁定中的方法'; 123456--先获取锁的信息select id,method_name,state,version from method_lock where state=1 and method_name='methodName';--占有锁update method_lock set state=2,version=2,update_time=now()where method_name='methodName' and state=1 and version=2;--如果没有更新影响到的这一行数据，说明这个资源已经被别人占位了。 1.3、这2种方案都是有弊端 缺点： 这把锁很依赖数据库的可用性，如果数据库是单个节点，一旦数据库挂了，则业务系统不可用了。 这把锁还没有失效时间，一旦解锁失败(更新state状态失败)，就会导致锁记录一直在数据库中，其他线程无法获取锁。 这把锁是非阻塞的，因为数是insert操作，一旦插入失败就会直接报错，没有获取锁的线程就不会进入到等待队列，要想再次获取锁就必须再次触发获取锁的操作。 这把锁是不可重入的，同一个线程在没有释放锁之前无法再次得到这个所，因为数据已经在数据库中存在了。 解决方案： 数据库是单点的话，可以数据库集群，然后数据之前进行同步，一旦挂掉，快速切换到备份库上。 没有失效时间，可以搞一个定时任务，每隔一段时间把数据库中的超时的数据delete。 他是非阻塞的，可以搞一个while循环，直到insert成功后再返回。 它是不可重入的，可以在数据库表上加一个字段，记录当前获取锁的机器的主机信息和线程信息，那么下次获取锁的时候可以先查询数据库，如果当前及其的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就行了。 2、基于Redis实现分布式锁12## 获取锁使用的命令：SET resource_name_value NX PX 30000 12345678910111213141516171819202122232425262728293031@Autowiredprivate RedisTemplate redisTemplate;private static final String LOCK=\"distribute_key\";public boolean test(String lockKey)&#123; Boolean lock=false; /** * 加锁的时候，在try块中，尝试获取锁，在finaly块中释放锁。 */ try &#123; //如果key不存在，则新增，如果存在则不改变已经有的值 lock = redisTemplate.opsForValue().setIfAbsent(lockKey, LOCK); log.info(\"test方法是否获取到锁：\" + lock); if (lock) &#123; //todo 业务逻辑 redisTemplate.expire(lockKey, 1, TimeUnit.MINUTES); return true; &#125; else &#123; log.info(\"test方法没有获取到锁，不执行任务！\"); &#125; &#125;finally &#123; if(lock)&#123; redisTemplate.delete(lockKey); log.info(\"test方法任务结束，释放锁！\"); &#125;else&#123; log.info(\"test方法没获取到锁，不需要释放锁！\"); &#125; return false; &#125;&#125; 2.1、弊端首先是这主从结构存在明显的竞争状态： 客户端A从master获取到锁，在master将锁同步到slave之前，master挂掉了，slave节点被晋级为master节点，客户端B取的了同一个资源被客户端A已经获取到的另外一个锁。安全失效。 3、基于Zookeeper实现​ Zookeeper中有节点的概念，Zk的数据存储结构就像一棵树，这棵树由节点组成，节点叫Znode，有点类似Linux的文件系统结构。 ​ Znode有4种类型： 持久节点Persistent 默认的节点类型，创建节点的客户端与zk断开连接后，这个节点仍然存在。 持久节点顺序节点Persistent_Sequential 顺序节点，就是在创建节点的时候，zk根据创建的时间顺序给该节点名称进行编号。 临时节点Ephemeral 和持久节点相反，当创建节点的客户端与zk断开后，临时节点会被删除。 临时顺序节点（Ephemeral_Sequential） 临时顺序节点就是结合了临时节点和顺序节点的特点：在创建临时节点的时候，zk根据创建的时间顺序给节点名称进行编号，当创建节点的客户端与zk断开连接后，临时顺序节点被删除。 3.1、Zookeeper分布式锁的原理Zk分布式锁使用的是临时顺序节点，也就是客户端断开连接后，会删除该节点，然后客户端创建节点的时候，会按照时间顺序来对临时节点命名。 获取锁 首先在zk中创建一个持久节点ParentLock，当第一个客户端想要获取锁的时候，需要在ParentLock这个节点下面创建临时顺序节点Lock1。 Client1查找ParnentLock下面所有的临时顺序节点并排序，然后判断自己创建的节点Lock1是不是顺序最靠前的一个节点。如果是第一个节点，就成功获取锁。 这个时候，再有一个客户端Client2前来获取锁，首先在ParentLock下再创建一个临时顺序节点Lock2 Client2查找ParnentLock下面所有的临时顺序节点并排序，判断它自己创建的节点Lock2是不是顺序最靠前的一个，结果发现节点Lock2并不是最小的。于是，Client2向排序仅仅比他靠前的节点Lock1注册Watcher，用来监听Lock1节点是否存在。这意味者此时Client2抢锁失败，进入了等待状态。 假如这个时候，又有一个客户端Client3前来获取锁，则在ParentLock下再创建一个临时顺序节点Lock3 Client3查找ParentLock下面所有的临时顺序节点并排序，判断自己所创建的节点Lock3是不是顺序最靠前的一个，结果同样发现节点Lock3并不是最小的。 于是，Client3向排序仅仅比他靠前的节点Lock2注册Watcher，用来监听Lock2节点是否存在，这意味这Client3同样抢锁失败，进入了等待状态。 这样就形成了，Client1获取了锁，Client2监听了Lock1，Client3监听额Lock2。从而形成了一个等待队列，很像Java中ReentrantLock所依赖的。 释放锁 1、任务完成，客户端显示释放 当任务完成后，Client1会显示调用删除节点Lock1的指令。 2、任务执行过程中，客户端崩溃 得到锁的Client1在执行任务过程中，突然挂了，则会断开与Zk服务端的连接，根据临时节点的特性，它自己创建的节点也会被自动删除。 由于Client2一直监听者Lock1的存在状态，当Lock1节点被删除后，Client2会立刻得到通知，这时候Client2会再次查ParentLock下面的所有节点，确认自己创建的节点Lock22是不是目前最小的节点，如果是最小，则Client2就成功获取了锁。 同理，如果Client2也因为任务完成或者节点崩溃而删除了Lock2，那么Client3会收到通知。 最终，Client3成功获取锁。 3.2、实现方法可以直接使用zk第三方库Curator客户端，这个客户端封装了一个可重入的锁服务。 123456789101112131415161718192021222324252627/** * InterProcessMutex是分布式锁的实现，acquire方法用户获取锁，release释放锁 */ @Autowired private InterProcessMutex interProcessMutex; public boolean tryLock(long timeout,TimeUnit unit)&#123; try &#123; interProcessMutex.acquire(timeout,unit); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; @Autowired private ExecutorCompletionService executorService; public boolean unLock()&#123; try &#123; interProcessMutex.release(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; //executorService.schedule(new Cleaner(client,path),delayTimeForClean,TimeUnit.MILLISECONDS); &#125; return true; &#125; 缺点： ​ 性能上，并没有缓存那么高，因为每次在创建锁和释放锁的过程中，都需要动态创建按，销毁临时顺序节点来实现锁功能。zk中创建和删除节点只能通过leader服务器来执行，然后将数据同步到所有的follwer服务器上。 ​ 使用zk也有可能带来并发问题，只是并不常见。由于网络抖动，客户端与zk集群的session连接断了，那么zk以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。也有可能产生并发问题，并不常见，因为zk有重试机制，一旦zk集群检测不到客户端的心跳，你就会重试，Curator客户端（操作zk的客户端）支持多种重试策略 ，多次重试之后还是不行的话，就会删除临时节点。因此，选择一个合适的重试策略也很重要，要在锁的粒度和并发之前找一个平衡。 5、总结 分布式锁 优点 缺点 Zk 1、有封装好的框架，容易实现。2、有等待锁的队列，大大提升抢锁效率。 添加和删除节点性能比较低 Redis Set和Del执行的性能比较高 1、实现复杂，需要考虑超时，原子性，误删的情况。2、没有等待锁的队列，只能在客户端自旋来等待，效率比较低。 6、3种方案比较使用3种方法，就如同CAP一样，在复杂性，可靠性，性能等方面无法同事满足。应该根据不同应用场景，选择最合适的分布式锁。 6.1、从理解的难易程度上，从低到高数据库——-&gt;Redis——-&gt;Zk 6.2、从实现的复杂程度上，从低到高zk————-&gt;Redis——-&gt;数据库 6.3、从性能角度，从低到高数据库————–&gt;zk———&gt;Redis 6.4、从可靠性角度，从低到高zk—————&gt;Redis—————&gt;数据库","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"Redis的使用","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、Redis简介/","text":"一、简介Redis是速度非常快的非关系型(NoSQL)内存键值数据库。可以存储键和5种不同类型的值之间的映射。 key的类型只能为字符串，value支持6中类型：String,Hash,List,Set,zset,HyperLogLog Redis支持很多特性，例如将内存中的数据持久化到硬盘中，使用复制来扩展读性能，使用分片来扩展写性能。 二、数据类型 数据类型 可以存储的值 操作 String 字符串，整数和浮点数 对整个字符串或者字符串的其中一部分执行操作，对整数和浮点数执行自增或者自减操作。 List 列表 从两端压入或者弹出元素，对单个或者多个元素进行修剪，只保留一个范围内的元素。 Set 无序集合 添加，获取，移除单个元素，检查一个元素是否存在于集合中，计算交集，并集，差集，从集合里面随机获取元素，去重。 Hash 包含键值对的无序散列表 添加，获取，移除单个键值对，获取素有键值对，检查某个键是否存在。 Zset 有序集合 添加，获取，删除元素，根据分值范围或者成员来获取元素计算一个键的排名 有点类似公司用的Wtable，每次用col key去获取数据。获取的数据里，有数据的，有字符串的，有对象的。 三、数据结构1、字典dictht是一个散列表结构，使用拉链法解决哈希冲突。 12345678910111213141516typedef struct dictht&#123; dictEntry **table; unsigned long size; unsigned long sizemask; unsigned long used;&#125;dictht;typedef struct dictEntry&#123; void *key; union&#123; void *val; uint64_t u64; double d; &#125;v; struct dictEntry *next;&#125;dictEntry; Redis的字典dict中包含两个哈希表dictht，只是为了方便进行rehash操作。在扩容的时候，将其中一个dictht上的键值对rehash到另一个dictht上面，完成之后释放空间并交换两个dictht的角色。 rehash操作不是一次性完成，而是采用渐进方式，这是为了避免一次性执行过多的rehash操作给服务器带来过大的负担。 渐进式rehash通过记录dict的rehashidx完成，他从0开始，然后每执行一次rehash都会递增。 2、跳跃表是有序集合的底层实现之一。 跳跃表是基于多指针有序链表实现的，可以看成多个有序链表。 在查找时候，从上层指针开始查找，找到对应的区间之后再到下一层去查找。 与红黑树等平衡树相比，跳跃表具有以下优点： 插入速度非常快，因为不需要进行旋转等操作来维护平衡性。 更容易实现 支持无锁操作。 四、Redis使用场景1、计数器可以对String进行自增自减运算，从而实现计数器功能。 Redis这种内存型数据库的读写性能非常高，很适合存储频繁读写的及数量。 2、缓存将热点数据存放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 3、查找表例如DNS的记录就很适合使用Redis进行存储。 查找表和缓存类似，也是利用Redis快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠地数据来源。 4、消息队列List是一个双向链表，可以通过lpush和rpop写入和读取消息 不过最好使用kafka，RabbitMQ等消息中间件。 5、Session缓存可以使用Redis来同一个存储多台应用服务器的Session信息。 当应用服务器不在存储用户的会话信息，也就不再具有状态。一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。 6、分布式锁实现在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。 可以使用Redis子代的SETNX命令实现分布式锁，除此之外，还可以使用官方提供的RedLock分布式锁实现。 7、其他Set可以实现交集，并集等操作，从而实现共同好友等功能 。 Zset可以shixi9an有序性操作，从而实现排行榜等功能。 五、Redis与Memcached两个都是非关系型内存键值数据库。主要区别如下： 1、数据类型Memcached只支持string类型，而Redis支持5种不同的数据类型，可更灵活的解决问题。 2、数据持久化Redis支持3种持久化策略：RDB快照和AOF日志，RDB和AOF混合型，而Memcached不支持持久化。 3、分布式Memcached不支持分布式，只能通过咋客户端使用一致性哈希来实现分布式存储，这种方式在存储和查询时都需要现在客户端计算一次数据所在的节点。 Redis Cluster实现了分布式的支持。 4、内存管理机制 在Redis中，并不是所有数据都是一直存在内存里，可以将一些很久没用的value交换到磁盘，而Memcached的数据则会一直在内存中。 Memcached将内存分割成特定长度的块来存储数据，以完全解决内存碎片的问题，但是这种方式会使得内存的利用率不高。比如块代销为128b，只存储100b的数据，那么剩下的28b就浪费掉了。 六、Redis的key过期时间Redis可以为每个key设置过期时间，当key过期时，会自动删除该key。 对于散列表这种容器，只能为整个key设置过期时间(整个散列表)，而不能为key里面的单个元素设置过期时间。 七、数据淘汰策略Redis可以设置内存最大使用量，当内存使用量超出时，会施行数据淘汰策略。 Redis具有6种淘汰策略： 策略 描述 volatile-LRU 当内存使用量超出时，此种策略，从已经设置过期时间的数据集中选出最近最少使用的数据淘汰 volatile-TTL 当内存使用量超出时，此种策略，从已经设置过期时间的数据集中选出将要过期的数据淘汰 volatile-Random 当内存使用量超出时，此种策略，从已经设置过期时间的数据中任意选择数据淘汰 allkeys-LRU 当内存使用量超出时，此种策略，从所有数据中挑选最近最少使用的数据淘汰。 allkeys-Random 当内存使用量超出时，此种策略，从所有数据中选任意数据进行淘汰 noeviction 禁止驱逐数据 作为内存数据库，处于对性能和内存消耗的考虑，Redis的淘汰算法实际实现上并非针对素有key，而是抽样一小部分并且从中选出被淘汰的key。 使用Redis缓存数据时，为了提高缓存命中率，需要保证缓存数据都是热点数据。可以将内存最大使用量设置为热点数据占用的内存量，然后启用allkeys-LRU淘汰策略，将最近最少使用的数据淘汰。 Redis4.0引入了volatile-LFU,allkeys-LFU淘汰策略，LFU策略通过统计访问频率，将访问频率最少的键值对淘汰。 八、持久化Redis是内存型数据库，为了保证数据在断电后不会丢失，需要将内存中的数据持久化到硬盘上。 1、RDB持久化 将某个时间点的所有数据都存到硬盘上 可以将快照复制到其他服务器从而创建具有相同数据的服务器副本。 如果系统发生故障，将会丢失最后一次创建快照之后的数据。 如果数据量很大，保存快照的时间会很长。 2、AOF持久化将写命令添加到AOF文件(Append Only File)的末尾。 使用AOF持久化需要设置同步选项，从而确保写命令同步到磁盘文件上的时机。这是因为对文件进行写入并不会马上将内容同步到磁盘上，而是先存储到缓冲区，然后由操作系统决定什么时候同步到磁盘。有以下同步选项： 选项 同步频率 always 每个写命令都同步 everysec 每秒同步一次 no 让操作系统决定何时同步 always选项会严重降低服务器的性能。 everysec选项比较合适，可以保证系统崩溃时只会丢失一秒左右的数据，并且Redis每秒执行一次同步对服务器性能几乎没有任何影响。 no选项并不能给服务器性能带来多大的提升，而且也会增加系统崩溃时数据丢失的数量。 随着服务器写请求的增多，AOF文件也会越来越大。Redis提供了一种将AOF重写特性，能够去除AOF文件中的冗余写命令。 九、事务一个事务包含了多个命令，服务器在执行事务期间，不会改去执行其他客户端的命令请求。也就是事务的ACID。 事务中的多个命令被一次性发给服务器，而不是一条一条的发送，这种方式叫流水线，它可以减少客户端和服务器之间的网络通信次数从而提升性能。 Redis最简单的事务实现方式是使用MULTI和EXEC命令来将事务操作包围起来。 十、事件Redis服务器是一个事件驱动程序。 1、文件事件","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"UDF自定义函数","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、UDF的使用手册/","text":"一、UDF的使用手册1、简介Hive中，提供了丰富的内置函数，比如trim(),cast(),max(),count(),coalesce()等之外， 还允许用户用java开发自定义的UDF函数。 1.1、开发自定义UDF函数的2种方式： 继承org.apache.hadoop.hive.ql.exec.UDF; 继承org.apache.hadoop.hive.ql.udf.generic.GenericUDF; 1.2、总结： 针对简单数据类型：String,Integer等，可以使用UDF。 针对复杂数据类型：Array,Map,Struct等，可以使用GenericUDF GenericUDF还可以在函数开始之前和结束之后做一些初始化和关闭的处理操作。 2、UDF使用UDF实现对String类型的字符串取HashMD5 12 3、GenericUDF","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Docker的使用","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/docker使用/","text":"一、docker的使用1、docker添加镜像加速文件123456789cd /etc/docker#如果没有daemon.json文件，需要新建一个sudo touch daemon.jsonsudo vim daemon.json#daemon.json添加以下配置，然后保存&#123;\"registry-mirrors\":[\"https://registry.docker-cn.com\"]&#125;#重启服务sudo systemctl daemon-reloadsudo systemctl restart docker 2、查看docker的版本1234567docker version#使用docker查找镜像sudo docker search mysql#查看本地下载的镜像sudo docker images#查看本地正在运行的镜像sudo docker ps -a 3、使用docker拉取镜像12345sudo docker pull redis:5.0.6sudo docker pull nginxsudo docker pull mysql:8.0.18#删除拉取的镜像sudo docker image rm mysql:8.0.18 4、使用docker安装并运行nginx12345678910111213141516sudo docker search nginx#pull的时候，后面不追加冒号和版本号，默认是最新的latest版本sudo docker pull nginx#运行nginxsudo docker run --name victor-nginx-test -p 8081:80 -d nginx#victor-nginx-test是容器名称-d，设置容器在后台一直运行-p，端口进行映射，将本地的8081端口映射到容器内部的80端口#执行玩上面那条命令后，会生成一串字符串45faac1c8dcac4c356e4fd8f8dc0ee204385feca7e851cbe874805db7b811f89，这个表示容器的id，一般作为日志的文件名来查看启动信息等sudo docker logs 45faac1c8dcac4c356e4fd8f8dc0ee204385feca7e851cbe874805db7b811f89#停止nginxsudo docker ps#复制对应容器的id,然后停止镜像sudo docker stop 45faac1c8dca#删除镜像sudo docker rm 45faac1c8dca 5、使用docker部署项目12345678910111213141516171819#首先创建一个文件夹，里面包含3个子文件夹sudo mkdir -p ~/桌面/nginx/www ~/桌面/nginx/logs ~/桌面/nginx/conf#首先你要直到容器的id，sudo docker ps -a#复制docker容器里的nginx默认配置文件信息，到你的本地刚创建的conf文件夹目录下sudo docker cp76d9d74d071e:/etc/nginx/nginx.conf ~/桌面/nginx/conf#部署项目，到docker里的nginxsudo docker run -p 8082:80 --name victor-nginx-web -v ~/桌面/nginx/www:/usr/share/nginx/html -v ~/桌面/nginx/conf/nginx.conf:/etc/nginx/nginx.conf -v ~/桌面/nginx/logs:/var/log/nginx -d nginx#命令说明-p 8082:80 是把你容器里的nginx的80端口映射到你本地主机的8082端口--name victor-nginx-web 是把容器重命名我们自己喜欢的名字-v ~/桌面/nginx/www:/usr/share/nginx/html 是把我们本地创建的项目 的www目录挂在到容器的/usr/share/nginx/html-v ~/桌面/nginx/conf/nginx.conf:/etc/nginx/nginx.conf 是吧我们创建的nginx.conf挂在到容器的/etc/nginx/nginx.conf-v ~/桌面/nginx/logs:/var/log/nginx 是把我们自己创建的logs挂在到容器的/var/log/nginx-d nginx :是设置容器一直后台运行#执行完命令后，会生成一串字符串，用这个字符串表示日志id，可以使用sudo docker logs 76d9d74d071e8b2d8499a56691587eef3e2f19d4fddeb5a3e26e5d9338856e10 6、使用docker安装mysql并运行123456789101112131415161718192021222324252627#拉去mysql的镜像sudo docker pull mysql:5.7#本地创建一个文件夹，用来映射容器的mysqlsudo mkdir -p ~/桌面/mydata/mysql/log ~/桌面/mydata/mysql/conf ~/桌面/mydata/mysql/data#给创建好的文件夹开权限sudo chmod 777 -R ~/桌面/mydata/#使用docker命令启动mysqlsudo docker run -p 3305:3306 --name mysql -v ~/桌面/mydata/mysql/log:/var/log/mysql -v ~/桌面/mydata/mysql/data:/var/lib/mysql -v ~/桌面/mydata/mysql/conf:/etc/mysql -e MYSQL_ROOT_PASSWORD=root -d mysql:5.7#查看镜像，如果存在，或者端口被占用，则换为3305映射容器的端口sudo docker ps -a#先删除被占用名字的容器镜像id，然后重新执行启动命令sudo docker rm3bf96e1f8a95#进入运行mysql的docker容器sudo docker exec -it mysql /bin/bash########以下是mysql的命令操作#使用mysql 命令打开客户端mysql -uroot -proot --default-character-set=utf8#查看数据库show databases;create database test character set utf8;use test;create table ();#退出mysql命令操作\\q#退出mysql运行的docker容器exit 7、使用docker安装并运行redis12345678910111213141516#docker拉取redissudo docker pull redis:3.2#本地创建一个redis文件夹存放数据sudo mkdir -p ~/桌面/mydata/redis/data#更改文件件执行权限sudo chmod 777 -R ~/桌面/mydata/redis#使用docker命令启动redissudo docker run -p 6379:6379 --name redis -v ~/桌面/mydata/redis/data:/data -d redis:3.2 redis-server --appendonly yes#进入redis容器使用redis-cli命令进行连接sudo docker exec -it redis redis-cli#####以下为redis的命令pingset a 100get a#退出redisexit 8、使用docker安装并运行rabbitmq123456789101112131415161718192021#拉取docker镜像sudo docker pull rabbitmq:3.7.15#使用docker命令启动rabbitmqsudo docker run --name rabbitmq --publish 5671:5671 --publish 5672:5672 --publish 4369:4369 --publish 25672:25672 --publish 15672:15672 --publish 15671:15671 -d rabbitmq:3.7.15#Ubuntu的防火墙暴露端口15672sudo ufw allow 15672#开启防火墙sudo ufw enable#重新加载sudo ufw reload#查看防火墙状态sudo ufw status#进入到rabbitmq的连接sudo docker exec -it rabbitmq /bin/bash#然后输入如下命令，开启rabbitmq的管理后台rabbitmq-plugins enable rabbitmq_management#浏览器输入rabbitmq的管理后台页面http://localhost:15672用户名密码：guest guest#退出exit","link":"","tags":[{"name":"Docker","slug":"Docker","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Docker/"},{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"Java并发编程","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、Java并发编程/","text":"一、Java并发编程 多线程程序包含两个或者多个可以同事运行的部分，每个部分可以同事处理不同的任务，从而能更好的利用可用资源，如果CPU是多核，多线程可以写入多个task，在同一个程序同时进行操作处理。 多任务是多个进程共享，比如CPU处理公共资源 多线程：将多任务的概念扩展到可以将单个应用程序中的特定操作细分为单个线程的应用程序 ，每个线程可以并行运行。 1、线程的生命周期线程在生命周期中经历多个阶段：线程诞生，启动，运行，然后死亡。 新线程New：新线程在新的状态下开始其生命周期，直到程序启动线程为止，保持在这种状态，也叫出生线程。 可运行Runnable：新诞生的线程启动以后，start，该线层可以运行，状态的线程被认为正在执行其任务。 等待Waiting：有时线程回转换到等待状态sleep,wait()，而线程等待另一个线程执行任务。只有当另一个线程发信号通知notify()/notifyAll()等待线程才能继续执行时，线程才转回到可运行状态。 定时等待Timed Waiting：可运行的线程可以在指定的时间间隔内进入定时等待状态，当该时间间隔sleep(100)到期或者发生等待的事件的时候，此状态的线程将转换到可运行状态。 终止dead：可执行线程在完成任务或者以其他方式终止的时候进入到终止状态。 2、线程优先级每个java线程都有一个优先级，可以帮助操作系统安排线程的顺序。Java线程优先级在MIN_PRIORITY(1),和MAX_PRIORITY(10)之间的范围内，默认情况下的，每个线程优先级都是NORM_PRIORITY(5) 具有较高的优先级的线程相对于一个程序来说更重要，应该在低优先级线程之前分配处理器时间。然而，线程优先级并不能呢保证线程执行的顺序。 3、通过实现Runnable接口创建线程 需要实现Runnable接口提供的run()方法。run()方法为线程提供一个入口，可以把完整的业务逻辑放到run()方法体。此方法是没有返回值的 123public void run()&#123; ...service....&#125; 然后在通过Thread构造函数，去创建一个对象 对象调用start()方法来启动线程。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253package multi_thread;/** * @Description:通过实现Runnable接口的方式创建线程 * 1、首先是需要实现Runnable的接口的run方法，run()作为线程提供一个入口，完成的业务逻辑写在run()里 * 2、通过Thread构造函数，去创建一个实例对象 * 3、对象调用start()方法来启动线程 * @Author: VictorDan * @Date: 19-8-12 下午3:58 * @Version: 1.0 */public class TestRunnable implements Runnable &#123; private Thread thread; private String threadName; public TestRunnable(String threadName) &#123; this.threadName = threadName; System.out.println(\"创建线程：\"+threadName); &#125; /** * 实现Runnable接口，需要重写run方法 * run方法里一般写的业务逻辑代码 */ @Override public void run() &#123; System.out.println(\"创建线程：\"+threadName); try &#123; for (int i = 0; i &lt; 5; i++) &#123; System.out.println(\"线程：\"+threadName+\",\"+i); Thread.sleep(50); &#125; &#125;catch(InterruptedException e)&#123; System.out.println(\"线程：\"+threadName+\"被打断\"); &#125; System.out.println(\"线程:\"+threadName+\"执行完退出\"); &#125; public void start()&#123; System.out.println(\"开启线程：\"+threadName); if(thread==null)&#123; thread=new Thread(this,threadName); thread.start(); &#125; &#125; public static void main(String[] args) &#123; TestRunnable thread1 = new TestRunnable(\"Thread-1\"); thread1.start(); TestRunnable thread2 = new TestRunnable(\"Thread-2\"); thread2.start(); &#125;&#125; 4、通过继承Thread类创建一个线程5、线程池的使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107 public static void main(String[] args) &#123; /** * 使用Executors可以创建线程池，但是你方便的同时，隐藏了复杂性，而埋下隐患，比如OOM，线程耗尽等。 * 所以阿里开发规范，创建线程池要手动创建，而不允许Executors直接调用的方式。 */ //1、创建一个不限制线程数上限的线程池，任何提交的任务都立即执行，最大上限是Integer.MAX_VALUE,会使用 Executors.newCachedThreadPool(); //2、创建一个固定个数大小的线程池 Executors.newFixedThreadPool(10); //3、创建一个只有1个线程的线程池 Executors.newSingleThreadExecutor(); //4、创建一个定时的线程池，工作队列是延迟队列。 Executors.newScheduledThreadPool(20); //总结：小程序使用这些快捷方法创建线程池没问题，但是对于服务端长期运行的程序，需要如下通过利用ThreadPoolExecutor的构造函数创建线程池。 /** * Executors.newXXXThreadPool(),这种方式创建线程池，会使用无界的任务队列，为了避免OOM，应该手动使用ThreadPoolExecutor的构造方法指定队列的最大长度。 * 还要明确拒绝任务时的策略，行为。也就是任务对了沾满的时候，这里在submit()新的任务， * public interface RejectedExecutionHandler&#123; * void rejectedExecution(Runnable r,ThreadPoolExecutor executor); * &#125; * 线程池提供了常见的拒绝策略。 * 1、new ThreadPoolExecutor.DiscardOldestPolicy():丢弃执行队列中最老的任务，尝试为当前提交的任务腾出位置 * 2、new ThreadPoolExecutor.DiscardPolicy()：什么都不做，直接忽略 * 3、new ThreadPoolExecutor.AbortPolicy();抛出RejectedExecutionException * 4、new ThreadPoolExecutor.CallerRunsPolicy();直接由提交任务者执行这个任务 * 线程池默认的拒绝行为是：AbortPolicy，也就是抛出RejectedExecutionException， * 如果不关心任务被拒绝，则可以使用DiscardPolicy，这样多余的任务可以悄悄的被忽略。 */ //Executors的方法也是使用ThreadPoolExecutor的构造方法创建的线程池。 new ThreadPoolExecutor(2, 20, 0L, SECONDS, new ArrayBlockingQueue&lt;&gt;(512), new ThreadPoolExecutor.DiscardOldestPolicy()); ExecutorService executorService = Executors.newCachedThreadPool(); new ThreadPoolExecutor.DiscardPolicy(); new ThreadPoolExecutor.AbortPolicy(); new ThreadPoolExecutor.CallerRunsPolicy(); new ThreadPoolExecutor.DiscardOldestPolicy(); //executorService=Executors.newSingleThreadExecutor(); for (int i = 0; i &lt; 20; i++) &#123; executorService.execute(new WorkTask()); &#125; Future&lt;Object&gt; future = executorService.submit(() -&gt; &#123; //这个异常会在调用Future.get()的时候传递给调用者 throw new RuntimeException(\"exception in call\"); &#125;); try &#123; Object result = future.get(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (ExecutionException e) &#123; //exception in Callable.call(); e.printStackTrace(); &#125; finally &#123; executorService.shutdown(); &#125; /** * 线程池的常用场景 * 1、如何正确的构造线程池 * 2、构造一个工作阻塞队列 * 3、设置拒绝策略 * 设置 */ int poolSize = Runtime.getRuntime().availableProcessors() * 2; /** * BlockingQueue只是一个接口，有好多实现类 * 1、ArrayBlockingQueue:底层基于数组实现的阻塞队列 * 2、LinkedBlockingQueue：底层基于链表实现的阻塞队列 * 3、LinkedBlockingDeque：底层基于双端链表实现的阻塞队列 * 4、DelayQueue：延迟队列 * 5、PriorityBlockingQueue:基于优先队列，底层是堆实现的。 * 6、SynchronousQueue:同步队列,队列内只存储一个元素。公平模式。 */ BlockingQueue&lt;Object&gt; queue = new ArrayBlockingQueue&lt;&gt;(512); RejectedExecutionHandler policy = new ThreadPoolExecutor.DiscardPolicy();// executorService = new ThreadPoolExecutor(poolSize, poolSize,// 0, TimeUnit.SECONDS,// queue,// policy); System.out.println(poolSize); &#125; class ThreadPoolExecutor1 &#123; /** * 线程池长期维持的线程数，即使没有任务，也不会回收 */ int corePoolSize; /** * 线程数的上限，比如一般是Integer.MAX_VALUE */ int maximumPoolSize; /** * 超过corePoolSize的线程，会有一个活跃时间，超过这个时间，多余线程会被回收 */ long keepAliveTime; TimeUnit unit; /** * 任务的排队队列，也就是排期队列。 */ BlockingQueue&lt;Runnable&gt; workQueue; /** * 创建线程的渠道 */ ThreadFactory threadFactory; /** * 拒绝策略，也就是任务队列也塞满了，新来的任务需要被拒绝。 */ RejectedExecutionHandler handler; &#125; 二、Dubbo的知识点dubbo是一款高性能，轻量级的开源java RPC框架，提供3大核心功能： 面向接口的远程方法调用 智能容错和负载均衡 服务自动注册和发现 dubbo是一个分布式服务框架，用来提供高性能和透明化的RPC远程服务调用方案，以及SOA服务治理。 1、什么是RPC？RPC：Remote Procedure Call，一个远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。比如说两个不同的服务A，B部署在两台不同的机器上，那么服务A如果想要调用服务B中的某个方法怎么解决？使用HTTP请求当然可以，但是可能会比较慢，而且一些优化做的并不好，RPC的出现就是为了解决这个问题。 RPC的原理： 服务消费方client调用以本地调用方式调用服务 client stub接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体。 client stub找到服务地址，并将消息发送到服务端 server stub收到消息后进行解码 server stub根据解码结果调用本地的服务 本地服务执行并结果返回给server stub server stub将返回结果打包成消息并发送到消费方 client stub接收到消息，并进行解码 服务消费方得到最终结果。 2、为什么要用dubbo？dubbo和SOA分布式架构的流行有着关系。SOA面向服务的架构(Service Oriented Architecture)，就是把工程按照业务逻辑拆分成服务层，表现层两个工程。服务层中包含业务逻辑，只需要对外提供服务就行。表现层只需要处理和页面的交互，业务逻辑都是调用服务层的服务来实现。SOA架构中有两个主要角色：服务提供者Provieder和服务使用者Consumer 3、开发分布式及程序，可以直接基于HTTP接口进行通信，但是为什么要使用dubbo？dubbo的四个特点： 负载均衡：同一个服务部署在不同的机器时该调用哪一台机器上的服务 服务调用链路生成：随着系统的发展，服务越来越多，服务间依赖关系变得错综复杂，甚至分不清哪个应用要在哪个应用之前启动，架构师都不能完整的描述应用的架构关系，dubbo可以为我们解决服务之间是如何互相调用的。 服务访问压力以及时长统计，资源调度和治理：基于访问压力实时管理集群容量，提供集群利用率。 服务降级：某个服务挂掉之后调用备用服务。 dubbo除了用在分布式系统中，也可以应用在微服务系统中，但是由于springboot cloud在微服务中更加广泛，所以一般提dubbo大部分在分布式系统的情况。 4、什么是分布式？分布式或者说SOA分布式重要的就是面向服务，说简单的分布式就是我们把整个系统拆分成不同的服务然后将这些服务放在不同的服务器上减轻单体服务的压力提高并发量和性能。比如电商系统可以简单拆分成订单系统，商品系统，登录系统等，拆分之后的每个服务可以部署在不同的机器上，如果某一个服务的访问量比较大的话也可以将这个服务同时部署在多台机器上。 5、为什么要分布式？从开发角度来讲单体应用的代码都集中在一起，而分布式系统的代码根据业务被拆分。所以每个团队可以负责一个服务的开发，这样提升了开发效率。另外，代码根据业务拆分之后更加便于维护和扩展。 另外，将系统拆分成分布式之后不光便于系统扩展和维护，更能提高整个系统的性能。把整个系统拆分成不同的服务、系统，然后每个服务，系统单独部署在一台服务器上，是不是很大成都提升了系统性能。 6、dubbo的架构 Provider：暴露服务的服务提供方 Consumer：调用远程服务的服务消费方 Registry：服务注册与发现的注册中心 Monitor：统计服务的调用次数和调用时间的监控中心 Container：服务运行容器 6.1、调用关系说明： Container服务容器负责启动，加载，运行服务提供者Provider。 服务提供者Provider在启动的是偶，向注册中心Registry注册自己提供的服务 服务消费者Consumer在启动的时候，向注册中心Registry订阅自己所需要的服务 注册中心Registry返回服务提供者Provider地址列表给消费者Consumer，如果有变更，注册中心Registry将基于长连接keep-alive推送变更数据给消费者Consumer 服务消费者Consumer，从服务提供者Provider地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。 dubbo提供了4中负载均衡算法： 权重随机算法：RandomLoadBalance 最少活跃调用数算法：LeastActiveLoadBalance 一致性哈希算法：ConsistentHashLoadBalance 加权轮询算法：RoundRobinLoadBalance 服务消费者Consumer和服务提供这Provider，在内存中累计调用次数和调用时间，定时每分钟发送一次统计数据到监控中心Monitor。 6.2、总结： 注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者Provider和服务消费者Consumer只在启动的时候与注册中心Registry交互，注册中心不转发请求，压力小。 监控中心Monitor负责统计各个服务调动次数，调用时间等，统计现在内存汇总后每分钟一次发送到监控中心服务器，并以报表的形式显示。 注册中心Registry，服务提供这Provider，服务消费者Consumer三者之间均为长连接Keep-alive，监控中心Monitor除外 注册中心Registry通过长连接感知服务提供者Provider的存在，服务提供者挂了，注册中心立即将推送事件通知消费者Consumer。 注册中心和监控中心全部挂了，不影响已经运行的服务提供者Provider和Consumer，消费者在本地缓存了提供者列表。 注册中心和监控中心都是可选的，服务消费者Consumer可以直接连服务提供者Provider 服务提供者Provider无状态，任意一台挂掉，不影响使用。 服务提供者Provider全部挂了，服务消费者Consumer应用将无法使用，并无限次重连等待服务提供者恢复。 6.3、Zookeeper挂了与dubbo直连的情况在实际生产中，如果zookeeper注册中心挂了，一段时间内服务消费者Consumer还是能够调用服务提供者Provider服务的，实际上Consumer使用了本地缓存进行通讯，Consumer本地缓存了提供者列表。 dubbo的健壮性: 监控中心挂了不影响使用","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"大数据","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、大数据BigData学习路线/","text":"一、大数据BigData1、简介​ 大数据本质也是数据，但是有了新的特征，包括数据来源广阔，数据格式多样化(结构化数据，非结构化数据，Excel文件，文本文件等) 数据量大(最少也是TB级别的，甚至可能是PB级别)，数据增长速度快。 2、需要考虑的问题 数据来源广，数据如何采集汇总？ 对应出现了Sqoop，Cammel，Datax等工具。 数据采集之后，如何存储？ 对应出现了GFS，HDFS，TFS等分布式文件存储系统 由于数据增长速度块，数据存储就必须水平扩展，于是出现集群 数据存储之后，该如何通过运算快速转化为统一的格式，该如何快速运算出自己想要的结果？ 对应的MapReduce，来解决这样的问题；但是写MapReduce需要写大量Java代码，所以出现了Hive，Pig等将SQL转化为MapReduce的解析引擎。 但是普通的MapReduce处理数据只能一批一批的执行处理Job跑脚本，时间延迟太长，是离线计算，跑的都是前一天的数据。为了实现实时处理，每输入一条数据就能得到结果，于是出现了Storm/JStorm这样的低延时的流式计算框架。 但是如果同时需要批处理和流处理，按照上面的，就需要搭建2个集群，一个Hadoop集群(包括HDFS+MapReduce+Yarn)和Storm集群。不便于管理，所以出现了Spark这样的一站式的计算框架，既可以进行批处理，也可以进行流处理(实质上是微批处理)。如今又进一步发展为Flink(快速流式处理计算框架)。 业务处理的通用架构是什么样的？ Lambda架构 Kappa机构出现 为了提高工作效率，加快运行速度，又出现了一些辅助工具 Ozzie，Azkaban：定时任务调度的工具 Hue，Zepplin：图形化任务执行管理，结果查看工具。类似公司的云窗系统 Scala语言：编写Spark程序的最佳语言，当然也可以使用Python或者Java Allluxio，Kylin：通过对存储的数据进行预处理，就快了运算速度的工具，能够快速的查询得到结果。 3、大数据的必备技能 日志收集：Flume，Fluentd，ELK 流式计算：Storm/JStorm，Spark Streaming，Flink Streaming 编程语言：Java，Python，Scala 机器学习库：Mahout，MLib 消息队列：Kafka，RabbitMQ 数据分析/数据仓库：Hive，SparkSQL，FlinkSQL，Pig，Kylin Hadoop家族：Zookeeper，HBase，Hue，Sqoop，Oozie 资源调度：Yarn，Mesos。 分布式数据存储：HDFS 大数据通用处理平台：Hadoop，Spark，Flink 4、必须掌握的技能 Java高级开发：JVM，并发 Linux基本操作 Hadoop（HDFS+MapReduce+Yarn） HBase(JavaAPI操作+Phoenix) Hive（Hql基本操作和原理理解） Kafka Storm Python Spark（Core+SparkSQL+Spark Streaming） 辅助小工具(Sqoop/Flume/jOzzie/Hue等)","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"大数据离线部分","date":"2019-11-01T02:06:00.000Z","path":"2019/11/01/一、大数据离线部分总结/","text":"一、大数据离线部分总结1、HDFS HDFS的架构部分以及工作原理 NameNode：负责管理元数据，将信息保存在内存中 DataNode：保存数据，以块的形式保存，启动后需要定时的向NameNode发送心跳，报告自身存储的Block块信息。 SecondNameNode：二级节点，负责合并请求。分担NameNode工作。 2、Spark容错机制分布式数据集的容错性有2种方式：数据检查和记录数据的更新。 面向大规模数据分析，数据检查点操作成本很高，需要通过数据中心的网络连接在机器之前复制庞大的数据集，而网络带宽往往比内存带宽低很多，同事还需要消耗更多的存储资源。 3、YARN的总结YARN（Yet Another Resource Negotiator）：另一种资源协调者。 3.1、使用YARN的背景​ 旧版本MapReduce中的JobTracker/TaskTracker在可扩展性、内存消耗、可靠性和线程模型方面存在很多问题，需要开发者做很多调整来修复。后来Hadoop开发者对这些问题进行了修复，可是也因而带来的成本却越来越高，为了从根本上解决旧版本的MapReduce存在的问题，从Hadoop 0.23.0版本开始，Hadoop的MapReduce框架就大改动。Hadoop新的MapReduce框架被叫做MapReduce V2，也叫YARN。 3.2、为什么要使用YARN​ 与旧版本的MapReduce比较，YARN采用了一种分层的集群框架。 解决了NameNode的单点故障问题，可以通过配置NameNode高可用来解决 提出了HDFS联邦，通过HDFS联邦可以让多个NameNode分别管理不同的目录，从而实现访问隔离以及横向扩展。 将资源管理和应用程序管理ApplicationManager分离开。用ResourceManager管理资源，用ApplicationMaster负责管理程序。（YARN的核心思想） 具有向后兼容特点，运行在MR1上的Job不需要做任何修改就可以运行在YARN上。 YARN是一个框架管理器，用户可以将各种计算框架移植到YARN上，统一由YARNJ进行管理和资源调度。目前YARN支持的计算框架有：MapReduce，Storm，Spark，Flink等。 3.2、YARN的基本架构 YARN的核心思想：将功能分开，ResourceManager进程完成整个集群的资源管理和调度。ApplicationMaster进程负责应用程序的相关事务，比如任务调度，容错，任务监控等。 系统中所有应用资源调度的最终决定权是ResourceManager担当的。 每个应用的ApplicationMaster实际上是框架指定的库，它从ResourceManager调度资源，和Node Manager一块执行监控任务。 NodeManager通过心跳信息向ResourceManager汇报自己所在节点的资源使用情况。 在旧版本的MapReduce(MR1)中，JobTracker有2个功能：一个是资源管理，另一个是作业调度。","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"MySQL的问题","date":"2019-10-08T02:06:00.000Z","path":"2019/10/08/开发中-MySQL的问题/","text":"MySQL的问题1、重置id问题由于在使用mysql，设计表的时候，设置了id自增，然后删除了数据后，再次新增数据时，就会出现id累计的情况，重置清空id，可以使用truncate 12#重置清空id，让id从1开始自增truncate table t_student 2、Insert ignore的使用表要求有：primary key，或者有unique索引 Insert ignore会忽略已存在的数据 1insert ignore into t_student(name,age,class) values(&quot;test&quot;,19,&quot;计算机&quot;);","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]},{"title":"Hive的使用","date":"2019-10-01T05:06:00.000Z","path":"2019/10/01/Hive/","text":"Hive Hive由Facebook实现并开源 是基于Hadoop的一个数据仓库工具 可以将结构化的数据映射为一张数据库表 提供HQL(Hive SQL)查询功能 底层数据是存在HDFS上 Hive的本质是将SQL语句转换为MapReduct任务进行 使得不熟悉MapReduce的用户很方便的利用HQL的处理和计算HDFS上的结构化的数据，适用于离线的批量数据计算。 数据仓库（Data Warehouse）是一个面向主题的，集成的Intergrated，相对稳定的Non-Volatile，反映历史变化的Time Variant的数据集合。 Hive依赖于HDFS存储数据，Hive将HQL转换成MapReduce执行，所以说Hive是基于Hadoop的一个数据仓库工具，实质上就是一个基于HDFS的MapReduce计算框架，对存储在HDFS中的数据进行分析和管理 。 使用Hive 直接使用MapReduce的问题： 人员学习成本太高 项目周期要求太短 MapReduce实现复杂查询逻辑的开发难度太大 使用Hive 更友好的接口：操作借口采用类SQL的语法，快速开发能力 学习成本低：避免了写MapReduce，减少开发学习成本 更好的扩展性：可自由扩展集群规模而不需要重启服务，用于还可以自定义函数 Hive使用场景 Hive与传统的关系型SQL不同，支持绝大多数的语句DDL，DML以及聚集函数，连接查询，条件查询。 Hive不适合联机事务处理，不提供实时查询功能，适用于基于大量不可变的批处理作业。 Hive的使用 不支持Insert into，Update，Delete操作 不支持等值连接 123select * from table a,table b where a.id=b.id;/*Hive中*/select * from table a join table b on a.id=b.id;","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"Hive的使用","date":"2019-10-01T02:06:00.000Z","path":"2019/10/01/Hive基本概念/","text":"Hive基本概念1.1、什么是HiveHive：由Facebook开源用于海量结构化日志的数据统计（Hive是分析框架，它不能存储数据的） Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。 本质：将HQL转化成MapReduce程序 数据仓库通过SQL进行统计分析 将SQL语言中常用的操作（select,where,group等）用MapReduce写成很多模板 所有的MapReduce模板封装在Hive中 client客户端，用户根据业务需求编写相应的SQL语句 然后会去Hive中找对应的MapReduce模板 通过Hive框架匹配出相应的MapReduce模板 运行MapReduce程序，生成相应的分析结果 Hive可以看做是Hadoop的客户端，装一个就够了啊，Hive它有不存储数据，不做计算，它就将HQL转化为MapReduce Hive处理的数据存储在HDFS Hive分析数据底层的实现是MapReduce 执行程序运行在Yarn上 1.2、Hive的优缺点1.2.1、优点 操作接口采用类SQL语法，提供快速开发的能力（简单，容易上手） 避免了去写MapReduce，减少开发人员得 学习成本 HIve的执行延迟比较高，因此HIve常用数据分析，对实时性要求不高的场合 比如你凌晨一两点，启动一个定时脚本，跑一下数据，实时性不高，不是那种你来一条数据，就立马处理的那种，就是说它可以处理，但是给反馈，交互出结果比较慢，因为它跑MR，而MR整个启动，提交啊，切片啊非常的慢，所以它应用的场景一般是离线 Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高，启动太慢 Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数 1.2.2、缺点1、Hive的HQL表达能力有限 迭代式算法无法表达 数据挖掘方面不擅长 2、Hive的效率比较低 Hive自动生成的MapReduce作业，通常情况下不够智能化 Hive调优比较困难，粒度比较粗 1.3、Hive架构原理 CLI：命令行接口，以命令行的形式输入SQL语句进行数据操作，有点类似shell Meta store：元数据存储，就是存的是表跟数据之间的位置对应关系，有点类似索引，Hive不会存储Meta store，会存在mysql或者Derby中 Meta store作用：客户端连接Meta store服务，Meta store再去连接Mysql数据库来存取元数据，有了Meta store服务，就可以有多个客户端可以同时连接，而切这些客户端不需要知道MySQL的数据库用户名和密码，只需要连接Meta store服务就行。 因为Meta store的元数据不断的修改，更新，所以Hive元数据不适合存在HDFS中，一般存在RDBMS中 HIve没有专门的数据存储格式，也没有为数据建立索引，Hive中所有数据都存储在HDFS中","link":"","tags":[{"name":"大数据","slug":"大数据","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"}]},{"title":"双指针","date":"2019-09-01T01:23:00.000Z","path":"2019/09/01/双指针/","text":"算法中的双指针使用双指针主要用来遍历数组，两个指针指向不同的元素，从而协同完成任务。 1、有序数组的Tow SumLeetCode的TowSum 题目描述：有序数组，找出两个数组，让和为target 算法思想：使用双指针，一个指针指向值较小的元素，一个指针指向值较大的元素，指向值较小元素的指针从头向后开始遍历，指向较大的指针从后向前开始遍历。 如果两个指针指向元素的和sum==target，返回结果位置 如果sum&gt;target，则移动较大的指针 如果sum&lt;target，则移动较小的指针 1234567891011121314public int[] twoSum(int[] numbers,int target)&#123; int i=0,j=numbers.length-1; while(i&lt;j)&#123; int sum=numbers[i]+numbers[j]; if(sum==target)&#123; return new int[]&#123;i+1,j+1&#125;; &#125;else if(sum&lt;target)&#123; i++; &#125;else&#123; j--; &#125; &#125; return null;&#125; 2、两数的平方和LeetCode 题目描述：判断一个数是否为两个数的平方和 算法思想：还是使用两个指针，一个指针为0，一个指针为和的sqrt 如果两个指针指向元素的和powSum==target，返回结果位置 如果powSum&gt;target，则移动较大的指针 如果powSum&lt;target，则移动较小的指针 1234567891011121314public boolean judgeSquerSum(int target)&#123; int i=0,j=(int)Math.sqrt(target); while(i&lt;=j)&#123;//此处有等号，不然target=2的时候，没有等号就返回false int sum=i*i+j*j; if(sum==target)&#123; return true; &#125;else if(sum&lt;target)&#123; i++; &#125;else&#123; j--; &#125; &#125; return false;&#125; 判断链表是否有环12345678910111213141516public class Solution &#123; public boolean hasCycle(ListNode head) &#123; if(head==null)&#123; return false; &#125; ListNode slow=head,fast=head;//声明两个临时指针，都指向head while(fast!=null&amp;&amp;fast.next!=null)&#123; slow=slow.next; fast=fast.next.next; if(slow==fast)&#123; return true; &#125; &#125; return false; &#125;&#125;","link":"","tags":[{"name":"Java","slug":"Java","permalink":"https://github.com/AnonymousDQ/victor.github.io/tags/Java/"}]}]